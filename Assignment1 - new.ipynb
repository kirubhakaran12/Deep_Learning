{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    os.chdir('C:\\\\Users\\\\s106961\\\\Desktop\\\\Deep Learning\\\\Data')\n",
    "    with h5py.File('train_128.h5','r') as H:\n",
    "        data = np.copy(H['data'])\n",
    "    with h5py.File('train_label.h5','r') as H:\n",
    "        label = np.copy(H['label'])\n",
    "    mu = data.mean()\n",
    "    sigma = data.std()\n",
    "    data = (data - mu) / sigma\n",
    "\n",
    "    return data, label\n",
    "\n",
    "data, label = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(X,y,iteration,batch_size = 100, shuffle = False):\n",
    "    dim = len(y)\n",
    "    idx = np.array(range(0,dim)).astype(np.int)\n",
    "    if shuffle == True: \n",
    "        np.random.shuffle(idx)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "    else:\n",
    "        start = (iteration * batch_size) % dim\n",
    "        end = start + batch_size\n",
    "        \n",
    "    X = X[idx[start:end]]\n",
    "    y = y[idx[start:end]]\n",
    "    return X, y\n",
    "\n",
    "def weight_init(X, y, hidden_units):\n",
    "    \n",
    "    Weights = {}\n",
    "    Bias = {}\n",
    "    no_hidden = len(hidden_units)\n",
    "    no_features = X.shape[1]\n",
    "    for n, units in enumerate(hidden_units):\n",
    "        Weights['layer_{}'.format(n+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)),high=np.sqrt(2. / (no_features)),size=(no_features, units))\n",
    "        Bias['layer_{}'.format(n+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)),high=np.sqrt(2. / (no_features)),size=(units))\n",
    "        no_features = units\n",
    "    \n",
    "    labels = len(np.unique(y))\n",
    "    Weights['layer_{}'.format(no_hidden+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)), high=np.sqrt(2. / (no_features)),size=(no_features, labels))\n",
    "    Bias['layer_{}'.format(no_hidden+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)),high=np.sqrt(2. / (no_features)),size=(labels))\n",
    "    \n",
    "    return Weights, Bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(inp):\n",
    "    return (1/(1 + np.exp(-inp)))\n",
    "\n",
    "def tanh(inp):\n",
    "    return (np.exp(inp) - np.exp(-inp))/(np.exp(inp) + np.exp(-inp))\n",
    "\n",
    "def relu(inp):\n",
    "    return np.maximum(inp,0)\n",
    "\n",
    "def softmax(inp):\n",
    "    #op_exp = np.exp(inp)\n",
    "    #return op_exp/op_exp.sum(axis = 1)[:,None]\n",
    "    \n",
    "    #the above code causes overflow. The below one is normalised.\n",
    "    inp_max = inp.max()\n",
    "    inp_norm = inp - inp_max\n",
    "    return np.exp(inp_norm) / np.sum(np.exp(inp_norm), axis=1) [:,None]\n",
    "\n",
    "def leaky_relu(inp):\n",
    "    return np.maximum(inp,0.1*inp)  \n",
    "   \n",
    "activation_fn = {'sigmoid': sigmoid, 'tanh': tanh, 'relu': relu, 'leaky_relu': leaky_relu}\n",
    "\n",
    "def gradient_sigmoid(inp):\n",
    "    return sigmoid(inp) * (1 - sigmoid(inp))\n",
    "\n",
    "def gradient_tanh(inp):\n",
    "    return (1 -tanh(inp)) * (1 + tanh(inp))\n",
    "\n",
    "def gradient_relu(inp):\n",
    "    inp[inp>0] = 1\n",
    "    inp[inp<0] = 0\n",
    "    return inp\n",
    "\n",
    "def gradient_leaky_relu(inp):\n",
    "    inp[inp<0] = 0.1\n",
    "    inp[inp>=0] = 1\n",
    "    return inp\n",
    "    \n",
    "gradient_fn = {'sigmoid': gradient_sigmoid, 'tanh': gradient_tanh, 'relu': gradient_relu, 'leaky_relu':gradient_leaky_relu}\n",
    "\n",
    "def output_label_hot_encode_old(inp):\n",
    "    inp = inp.astype(int)\n",
    "    classes = np.unique(inp)\n",
    "    no_classes = classes.shape[0]\n",
    "    no_dim = inp.shape[0]\n",
    "    zero_arr = np.zeros((no_dim, no_classes))\n",
    "    zero_arr[np.arange(no_dim), inp] = 1.0\n",
    "    return zero_arr\n",
    "\n",
    "def output_label_hot_encode(inp,batch_size,no_label):\n",
    "    one_hot_array = np.zeros((batch_size,no_label))\n",
    "    one_hot_array[np.arange(batch_size),inp] = 1\n",
    "    return one_hot_array\n",
    "\n",
    "def cross_entropy_error(y_act, y_hat, Weights, regularization):\n",
    "    regularized_loss = 0\n",
    "    for layer in Weights:\n",
    "        regularized_loss += np.sum(np.square(Weights[layer]))\n",
    "    return np.sum(np.multiply(y_act, np.log(y_hat)) * -1) + (0.5 * regularization  * regularized_loss)\n",
    "\n",
    "def accuracy(y_hat, y_act):\n",
    "    return round((np.sum((y_hat.argmax(axis=1) == y_act.argmax(axis=1)))/y_act.shape[0] * 100),2)\n",
    "\n",
    "def predict(data, label, Weights, Bias, no_hidden = 2, activation = 'relu'):\n",
    "    layers, y_hat = forward_propagation(data, label, Weights, Bias, no_hidden, activation,dropout_units = None, predict =True)\n",
    "    y_act = output_label_hot_encode(label, data.shape[0], no_label = 10)\n",
    "    return y_hat, y_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, y, Weights, Bias, no_hidden, activation, dropout_units, predict = False):\n",
    "    \n",
    "    layers = {}\n",
    "    layers['layer_{}'.format(0)] = X #input layer\n",
    "    if predict == False:\n",
    "        layers['layer_{}'.format(0)][:,dropout_units['layer_{}'.format(0)]] = 0\n",
    "\n",
    "    for l in range(no_hidden):\n",
    "        layers['layer_wx_{}'.format(l+1)] = np.matmul(layers['layer_{}'.format(l)], Weights['layer_{}'.format(l+1)]) + Bias['layer_{}'.format(l+1)]\n",
    "        layers['layer_{}'.format(l+1)] = activation_fn[activation](layers['layer_wx_{}'.format(l+1)])\n",
    "        \n",
    "        if predict == False:\n",
    "            layers['layer_wx_{}'.format(l+1)][:,dropout_units['layer_{}'.format(l+1)]] = 0\n",
    "            layers['layer_{}'.format(l+1)][:,dropout_units['layer_{}'.format(l+1)]] = 0\n",
    "        \n",
    "    layers['layer_wx_{}'.format(no_hidden+1)] = np.matmul(layers['layer_{}'.format(no_hidden)], Weights['layer_{}'.format(no_hidden+1)])\n",
    "    layers['layer_{}'.format(no_hidden+1)] = softmax(layers['layer_wx_{}'.format(no_hidden+1)])\n",
    "    \n",
    "    y_hat = layers['layer_{}'.format(no_hidden+1)]\n",
    "        \n",
    "    return (layers, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(layers, y_act, y_hat, Weights, Bias, no_hidden, activation, regularization):\n",
    "    \n",
    "    gradients = {}\n",
    "    dim = y_act.shape[0]\n",
    "    \n",
    "    gradients['dL/dA_layer_{}'.format(no_hidden+1)]  = ((y_act - y_hat) * -1)/dim\n",
    "    \n",
    "    for i in range(no_hidden,-1,-1):\n",
    "        \n",
    "        gradients['dL/dW_layer{}'.format(i+1)] = np.zeros(Weights['layer_{}'.format(i+1)].shape)\n",
    "        for j in range(Weights['layer_{}'.format(i+1)].shape[0]):\n",
    "            for k in range(Weights['layer_{}'.format(i+1)].shape[1]):\n",
    "                gradients['dL/dW_layer{}'.format(i+1)][j][k] = np.sum((np.multiply(gradients['dL/dA_layer_{}'.format(i+1)][:,k], layers['layer_{}'.format(i)][:,j]))) + (regularization * Weights['layer_{}'.format(i+1)][j][k])\n",
    "                \n",
    "        gradients['dL/dB_layer{}'.format(i+1)] = np.sum(gradients['dL/dA_layer_{}'.format(i+1)],axis=0)\n",
    "        if i != 0:\n",
    "            gradients['dL/dH_layer{}'.format(i)] = np.matmul(gradients['dL/dA_layer_{}'.format(i+1)],np.transpose(Weights['layer_{}'.format(i+1)]))\n",
    "            gradients['dL/dA_layer_{}'.format(i)] = gradient_fn[activation](layers['layer_wx_{}'.format(i)]) * gradients['dL/dH_layer{}'.format(i)]\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(gradients, Weights, Bias, learning_rate,no_hidden):\n",
    "    for i in range(no_hidden+1,0,-1):\n",
    "        Weights['layer_{}'.format(i)] = Weights['layer_{}'.format(i)] - (learning_rate * gradients['dL/dW_layer{}'.format(i)])\n",
    "        Bias['layer_{}'.format(i)] = Bias['layer_{}'.format(i)] - (learning_rate * gradients['dL/dB_layer{}'.format(i)])\n",
    "        \n",
    "    return Weights, Bias\n",
    "\n",
    "def momentum_gradient_descent(gradients, Weights, Bias, learning_rate,no_hidden, rho, prev_gradients):\n",
    "    momentum_gradient = {}\n",
    "    for i in range(no_hidden+1,0,-1):\n",
    "        momentum_gradient['dL/dW_layer{}'.format(i)] = (rho * prev_gradients['dL/dW_layer{}'.format(i)]) + (learning_rate * gradients['dL/dW_layer{}'.format(i)])\n",
    "        momentum_gradient['dL/dB_layer{}'.format(i)] = (rho * prev_gradients['dL/dB_layer{}'.format(i)]) + (learning_rate * gradients['dL/dB_layer{}'.format(i)])\n",
    "        Weights['layer_{}'.format(i)] = Weights['layer_{}'.format(i)] - momentum_gradient['dL/dW_layer{}'.format(i)]\n",
    "        Bias['layer_{}'.format(i)] = Bias['layer_{}'.format(i)] - momentum_gradient['dL/dB_layer{}'.format(i)]\n",
    "        \n",
    "    return Weights, Bias, momentum_gradient\n",
    "\n",
    "def adam(gradients, Weights, Bias, learning_rate,no_hidden, prev_gradients, prev_velocity, beta1, beta2, time):\n",
    "    momentum_gradient = {}\n",
    "    velocity = {}\n",
    "    ephsilon = 1e-5\n",
    "    for i in range(no_hidden+1,0,-1):\n",
    "        momentum_gradient['dL/dW_layer{}'.format(i)] = ((beta1 * prev_gradients['dL/dW_layer{}'.format(i)]) + ((1-beta1) * gradients['dL/dW_layer{}'.format(i)]))#/(1-beta1**time)\n",
    "        momentum_gradient['dL/dB_layer{}'.format(i)] = ((beta1 * prev_gradients['dL/dB_layer{}'.format(i)]) + ((1-beta1) * gradients['dL/dB_layer{}'.format(i)]))#/(1-beta1**time)\n",
    "        velocity['dL/dW_layer{}'.format(i)] = (beta2 * prev_velocity['dL/dW_layer{}'.format(i)]) + ((1-beta2) * np.square(gradients['dL/dW_layer{}'.format(i)]))#/(1-beta2**time)\n",
    "        velocity['dL/dB_layer{}'.format(i)] = (beta2 * prev_velocity['dL/dB_layer{}'.format(i)]) + ((1-beta2) * np.square(gradients['dL/dB_layer{}'.format(i)]))#/(1-beta2**time)\n",
    "        \n",
    "        Weights['layer_{}'.format(i)] = Weights['layer_{}'.format(i)] - ((learning_rate * momentum_gradient['dL/dW_layer{}'.format(i)])/(np.sqrt(velocity['dL/dW_layer{}'.format(i)]) + ephsilon))\n",
    "        Bias['layer_{}'.format(i)] = Bias['layer_{}'.format(i)] - ((learning_rate * momentum_gradient['dL/dB_layer{}'.format(i)])/(np.sqrt(velocity['dL/dB_layer{}'.format(i)]) + ephsilon))\n",
    "\n",
    "    return Weights, Bias, momentum_gradient, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(input_features, hidden_units, input_layer_ratio = 0.8, hidden_layer_ratio = 0.5):\n",
    "    \n",
    "    dropout_units = {}\n",
    "    drop_units = np.arange(input_features)\n",
    "    np.random.shuffle(drop_units)\n",
    "    dropout_units['layer_{}'.format(0)] = drop_units[:int(input_features * (1-input_layer_ratio))]\n",
    "\n",
    "    for (i, units) in enumerate(hidden_units,1):\n",
    "        drop_units = np.arange(units)\n",
    "        np.random.shuffle(drop_units)\n",
    "        dropout_units['layer_{}'.format(i)] = drop_units[:int(units * (1-hidden_layer_ratio))]\n",
    "    return dropout_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, label, hidden_units, Weights, Bias, activation = 'sigmoid', descent_method = 'momentum_gradient_descent', \n",
    "          epochs = 10000, learning_rate = 0.01,batch_size = 100):\n",
    "    \n",
    "    start = time.time()    \n",
    "    error = []   \n",
    "    no_hidden = len(hidden_units)\n",
    "    no_label = len(np.unique(label))\n",
    "    input_features = data.shape[1]\n",
    "    rho = 0.9\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    regularization = 0\n",
    "    prev_accuracy = 0\n",
    "    curr_accuracy = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        dropout_units = dropout(input_features, hidden_units, input_layer_ratio = 0.8, hidden_layer_ratio = 0.5)\n",
    "        X, y = mini_batch(data,label,i,batch_size,shuffle = False)\n",
    "        y_act = output_label_hot_encode(y, batch_size, no_label)\n",
    "        layers, y_hat = forward_propagation(X, y, Weights, Bias, no_hidden, activation, dropout_units)\n",
    "        gradients = backward_propagation(layers, y_act, y_hat, Weights, Bias, no_hidden, activation, regularization)\n",
    "        \n",
    "        if descent_method == 'gradient_descent':\n",
    "            Weights, Bias = gradient_descent(gradients, Weights, Bias, learning_rate, no_hidden)\n",
    "        elif descent_method == 'momentum_gradient_descent':\n",
    "            if i == 0:\n",
    "                prev_gradients = {layer:gradients[layer] * 0 for layer in gradients}\n",
    "            Weights, Bias, prev_gradients = momentum_gradient_descent(gradients, Weights, Bias, learning_rate, no_hidden,\n",
    "                                                                      rho, prev_gradients)\n",
    "        elif descent_method == 'adam':\n",
    "            if i == 0:\n",
    "                prev_gradients = {layer:gradients[layer] * 0 for layer in gradients}\n",
    "                prev_velocity = {layer:gradients[layer] * 0 for layer in gradients}\n",
    "            Weights, Bias, prev_gradients, prev_velocity = adam(gradients, Weights, Bias, learning_rate,no_hidden, \n",
    "                                                  prev_gradients, prev_velocity, beta1, beta2, time = i+1)\n",
    "            \n",
    "        ce_error = cross_entropy_error(y_act, y_hat, Weights, regularization)\n",
    "        \n",
    "        pred_y_hat, pred_y_act = predict(data, label, Weights, Bias, no_hidden, activation = 'relu')\n",
    "        curr_accuracy = accuracy(pred_y_hat, pred_y_act)\n",
    "        \n",
    "        #if prev_accuracy > curr_accuracy:\n",
    "        #    Weights = prev_Weights\n",
    "        #else:\n",
    "        #    prev_accuracy = curr_accuracy\n",
    "        #    prev_Weights = Weights\n",
    "            \n",
    "        if i%100 == 0: error.append(ce_error)\n",
    "        if i%100 == 0: \n",
    "            print('Epoch:' + str(i) + ' Error:' + str(round(ce_error,2)) + ' Accuracy:' + str(curr_accuracy))\n",
    "        \n",
    "    end = time.time()\n",
    "    print('Training time: ' + str(round((end - start)/60)) + ' mins')\n",
    "    return error, y_act, y_hat, Weights, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [100,50]\n",
    "no_hidden = len(hidden_units)\n",
    "Weights, Bias = weight_init(data, label, hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Error:231.2 Accuracy:28.72\n",
      "Epoch:100 Error:108.72 Accuracy:69.68\n",
      "Epoch:200 Error:164.77 Accuracy:72.54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-503-e6608fe24950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m error, y_act, y_hat, Weight_new, iters = train(data, label, hidden_units, Weights, Bias, activation = 'relu',\n\u001b[0;32m      2\u001b[0m                                                \u001b[0mdescent_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                                                learning_rate = 0.01,batch_size = 100)\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpred_y_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_y_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWeights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_y_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_y_act\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-501-6e30d0f934a7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data, label, hidden_units, Weights, Bias, activation, descent_method, epochs, learning_rate, batch_size)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mce_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_act\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWeights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mpred_y_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_y_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWeights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mcurr_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_y_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_y_act\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-470-9e08e8d5c01f>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(data, label, Weights, Bias, no_hidden, activation)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWeights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWeights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdropout_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[0my_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_label_hot_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_act\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-471-854bad0eff00>\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(X, y, Weights, Bias, no_hidden, activation, dropout_units, predict)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer_wx_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWeights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mBias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer_wx_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "error, y_act, y_hat, Weight_new, iters = train(data, label, hidden_units, Weights, Bias, activation = 'relu',\n",
    "                                               descent_method = 'adam', epochs = 5000, \n",
    "                                               learning_rate = 0.01,batch_size = 100)\n",
    "pred_y_hat, pred_y_act = predict(data, label, Weights, Bias, no_hidden, activation = 'relu')\n",
    "print('Accuracy ' + str(accuracy(pred_y_hat, pred_y_act)))\n",
    "plt.plot(error)\n",
    "plt.ylabel('Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 80.91\n"
     ]
    }
   ],
   "source": [
    "pred_y_hat, pred_y_act = predict(data, label, Weights, Bias, no_hidden, activation = 'relu')\n",
    "print('Accuracy ' + str(accuracy(pred_y_hat, pred_y_act)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
