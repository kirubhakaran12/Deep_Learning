{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    os.chdir('C:\\\\Users\\\\s106961\\\\Desktop\\\\Deep Learning\\\\Data')\n",
    "    with h5py.File('train_128.h5','r') as H:\n",
    "        data = np.copy(H['data'])\n",
    "    with h5py.File('train_label.h5','r') as H:\n",
    "        label = np.copy(H['label'])\n",
    "    mu = data.mean()\n",
    "    sigma = data.std()\n",
    "    data = (data - mu) / sigma\n",
    "\n",
    "    return data, label\n",
    "\n",
    "data, label = preprocess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(X,y, batch_size = 100):\n",
    "    idx = np.array(range(0,len(y))).astype(np.int)\n",
    "    np.random.shuffle(idx)\n",
    "    X = X[idx[:batch_size]]\n",
    "    y = y[idx[:batch_size]]\n",
    "    return X, y\n",
    "\n",
    "def weight_init(X, y, hidden_units):\n",
    "    \n",
    "    Weights = {}\n",
    "    Bias = {}\n",
    "    no_hidden = len(hidden_units)\n",
    "    no_features = X.shape[1]\n",
    "    for n, units in enumerate(hidden_units):\n",
    "        Weights['layer_{}'.format(n+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)),high=np.sqrt(2. / (no_features)),size=(no_features, units))\n",
    "        Bias['layer_{}'.format(n+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)),high=np.sqrt(2. / (no_features)),size=(units))\n",
    "        no_features = units\n",
    "    \n",
    "    labels = len(np.unique(y))\n",
    "    Weights['layer_{}'.format(no_hidden+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)), high=np.sqrt(2. / (no_features)),size=(no_features, labels))\n",
    "    Bias['layer_{}'.format(no_hidden+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)),high=np.sqrt(2. / (no_features)),size=(labels))\n",
    "    \n",
    "    return Weights, Bias \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(inp):\n",
    "    return (1/(1 + np.exp(-inp)))\n",
    "\n",
    "def tanh(inp):\n",
    "    return (np.exp(inp) - np.exp(-inp))/(np.exp(inp) + np.exp(-inp))\n",
    "\n",
    "def relu(inp):\n",
    "    return np.maximum(inp,0)\n",
    "\n",
    "def softmax(inp):\n",
    "    #op_exp = np.exp(inp)\n",
    "    #return op_exp/op_exp.sum(axis = 1)[:,None]\n",
    "    \n",
    "    #the above code causes overflow. The below one is normalised.\n",
    "    inp_max = inp.max()\n",
    "    inp_norm = inp - inp_max\n",
    "    return np.exp(inp_norm) / np.sum(np.exp(inp_norm), axis=1) [:,None]\n",
    "\n",
    "activation_fn = {'sigmoid': sigmoid, 'tanh': tanh, 'relu': relu}\n",
    "\n",
    "def gradient_sigmoid(inp):\n",
    "    return sigmoid(inp) * (1 - sigmoid(inp))\n",
    "\n",
    "def gradient_tanh(inp):\n",
    "    return (1 -tanh(inp)) * (1 + tanh(inp))\n",
    "\n",
    "def gradient_relu(inp):\n",
    "    inp[inp>0] = 1\n",
    "    inp[inp<0] = 0\n",
    "    return inp\n",
    "\n",
    "gradient_fn = {'sigmoid': gradient_sigmoid, 'tanh': gradient_tanh, 'relu': gradient_relu}\n",
    "\n",
    "def output_label_hot_encode_old(inp):\n",
    "    inp = inp.astype(int)\n",
    "    classes = np.unique(inp)\n",
    "    no_classes = classes.shape[0]\n",
    "    no_dim = inp.shape[0]\n",
    "    zero_arr = np.zeros((no_dim, no_classes))\n",
    "    zero_arr[np.arange(no_dim), inp] = 1.0\n",
    "    return zero_arr\n",
    "\n",
    "def output_label_hot_encode(inp,batch_size,no_label):\n",
    "    one_hot_array = np.zeros((batch_size,no_label))\n",
    "    one_hot_array[np.arange(batch_size),inp] = 1\n",
    "    return one_hot_array\n",
    "\n",
    "def cross_entropy_error(y_act, y_hat):\n",
    "    return np.sum(np.multiply(y_act, np.log(y_hat)) * -1)\n",
    "\n",
    "def accuracy(y_hat, y_act):\n",
    "    return round((np.sum((y_hat.argmax(axis=1) == y_act.argmax(axis=1)))/y_act.shape[0] * 100),2)\n",
    "\n",
    "def predict(data, label, Weights, Bias, no_hidden = 2, activation = 'relu'):\n",
    "    layers, y_hat = forward_propagation(data, label, Weights, Bias, no_hidden, activation)\n",
    "    y_act = output_label_hot_encode(label, data.shape[0], no_label = 10)\n",
    "    return y_hat, y_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, y, Weights, Bias, no_hidden, activation):\n",
    "    \n",
    "    layers = {}\n",
    "    layers['layer_{}'.format(0)] = X #input layer\n",
    "\n",
    "    for l in range(no_hidden):\n",
    "        layers['layer_wx_{}'.format(l+1)] = np.matmul(layers['layer_{}'.format(l)], Weights['layer_{}'.format(l+1)]) + Bias['layer_{}'.format(l+1)]\n",
    "        layers['layer_{}'.format(l+1)] = activation_fn[activation](layers['layer_wx_{}'.format(l+1)])\n",
    "        \n",
    "    layers['layer_wx_{}'.format(no_hidden+1)] = np.matmul(layers['layer_{}'.format(no_hidden)], Weights['layer_{}'.format(no_hidden+1)])\n",
    "    layers['layer_{}'.format(no_hidden+1)] = softmax(layers['layer_wx_{}'.format(no_hidden+1)])\n",
    "    \n",
    "    y_hat = layers['layer_{}'.format(no_hidden+1)]\n",
    "        \n",
    "    return (layers, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(layers, y_act, y_hat, Weights, Bias, no_hidden, activation):\n",
    "    \n",
    "    gradients = {}\n",
    "    dim = y_act.shape[0]\n",
    "    \n",
    "    gradients['dL/dA_layer_{}'.format(no_hidden+1)]  = ((y_act - y_hat) * -1)/dim\n",
    "    \n",
    "    for i in range(no_hidden,-1,-1):\n",
    "        \n",
    "        gradients['dL/dW_layer{}'.format(i+1)] = np.zeros(Weights['layer_{}'.format(i+1)].shape)\n",
    "        for j in range(Weights['layer_{}'.format(i+1)].shape[0]):\n",
    "            for k in range(Weights['layer_{}'.format(i+1)].shape[1]):\n",
    "                gradients['dL/dW_layer{}'.format(i+1)][j][k] = np.sum((np.multiply(gradients['dL/dA_layer_{}'.format(i+1)][:,k], layers['layer_{}'.format(i)][:,j])))\n",
    "                \n",
    "        gradients['dL/dB_layer{}'.format(i+1)] = np.sum(gradients['dL/dA_layer_{}'.format(i+1)],axis=0)\n",
    "        if i != 0:\n",
    "            gradients['dL/dH_layer{}'.format(i)] = np.matmul(gradients['dL/dA_layer_{}'.format(i+1)],np.transpose(Weights['layer_{}'.format(i+1)]))\n",
    "            gradients['dL/dA_layer_{}'.format(i)] = gradient_fn[activation](layers['layer_wx_{}'.format(i)]) * gradients['dL/dH_layer{}'.format(i)]\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(gradients, Weights, Bias, learning_rate,no_hidden):\n",
    "    for i in range(no_hidden+1,0,-1):\n",
    "        Weights['layer_{}'.format(i)] = Weights['layer_{}'.format(i)] - (learning_rate * gradients['dL/dW_layer{}'.format(i)])\n",
    "        Bias['layer_{}'.format(i)] = Bias['layer_{}'.format(i)] - (learning_rate * gradients['dL/dB_layer{}'.format(i)])\n",
    "        \n",
    "    return Weights, Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, label, hidden_units, Weights, Bias, activation = 'sigmoid', epochs = 10000, learning_rate = 0.01,batch_size = 100):\n",
    "    error = []   \n",
    "    no_hidden = len(hidden_units)\n",
    "    no_label = len(np.unique(label))\n",
    "\n",
    "    for i in range(epochs):\n",
    "        X, y = mini_batch(data,label,batch_size)\n",
    "        y_act = output_label_hot_encode(y, batch_size, no_label)\n",
    "        layers, y_hat = forward_propagation(X, y, Weights, Bias, no_hidden, activation)\n",
    "        gradients = backward_propagation(layers, y_act, y_hat, Weights, Bias, no_hidden, activation)\n",
    "        Weights, Bias = update_params(gradients, Weights, Bias, learning_rate, no_hidden)\n",
    "        ce_error = cross_entropy_error(y_act, y_hat)\n",
    "        if i%100 == 0: error.append(ce_error)\n",
    "        if i%1000 == 0: \n",
    "            pred_y_hat, pred_y_act = predict(data, label, Weights, Bias, no_hidden, activation = 'relu')\n",
    "            print('Epoch:' + str(i) + ' Error:' + str(round(ce_error,2)) + ' Accuracy:' + str(accuracy(pred_y_hat, pred_y_act)))\n",
    "        #if ce_error < 1:\n",
    "        #    break\n",
    "    return error, y_act, y_hat, Weights, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2389,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [4,4,3]\n",
    "no_hidden = len(hidden_units)\n",
    "Weights, Bias = weight_init(data, label, hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Error:232.56 Accuracy:15.22\n",
      "Epoch:1000 Error:194.16 Accuracy:18.42\n",
      "Epoch:2000 Error:157.0 Accuracy:28.37\n",
      "Epoch:3000 Error:144.44 Accuracy:30.31\n",
      "Epoch:4000 Error:112.43 Accuracy:34.37\n",
      "Epoch:5000 Error:128.39 Accuracy:42.34\n",
      "Epoch:6000 Error:125.72 Accuracy:45.15\n",
      "Epoch:7000 Error:108.48 Accuracy:45.92\n"
     ]
    }
   ],
   "source": [
    "error, y_act, y_hat, Weight_new, iters = train(data, label, hidden_units, Weights, Bias, activation = 'relu', epochs = 10000, learning_rate = 0.01,batch_size = 100)\n",
    "pred_y_hat, pred_y_act = predict(data, label, Weights, Bias, no_hidden, activation = 'relu')\n",
    "print('Accuracy ' + str(accuracy(pred_y_hat, pred_y_act)))\n",
    "plt.plot(error)\n",
    "plt.ylabel('Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 9.3\n"
     ]
    }
   ],
   "source": [
    "pred_y_hat, pred_y_act = predict(data, label, Weights, Bias, no_hidden = 2, activation = 'relu')\n",
    "print('Accuracy ' + str(accuracy(pred_y_hat, pred_y_act)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
