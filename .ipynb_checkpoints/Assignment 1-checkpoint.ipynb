{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "size = 100\n",
    "x11 = np.random.uniform(low=0.0,high=1.0,size=size)\n",
    "x12 = np.random.uniform(low=2.0,high=8.0,size=size)\n",
    "x13 = np.random.uniform(low=6.0,high=8.0,size=size)\n",
    "\n",
    "x21 = np.random.uniform(low=10.0,high=11.0,size=size)\n",
    "x22 = np.random.uniform(low=12.0,high=18.0,size=size)\n",
    "x23 = np.random.uniform(low=16.0,high=18.0,size=size)\n",
    "\n",
    "x31 = np.random.uniform(low=20.0,high=21.0,size=size)\n",
    "x32 = np.random.uniform(low=22.0,high=28.0,size=size)\n",
    "x33 = np.random.uniform(low=26.0,high=28.0,size=size)\n",
    "\n",
    "y1 = np.transpose(np.zeros(100))\n",
    "y2 = np.transpose(np.ones(100))\n",
    "y3 = np.transpose(np.ones(100) * 2)\n",
    "\n",
    "X1 = np.transpose(np.array([x11, x12, x13]))\n",
    "X2 = np.transpose(np.array([x21, x22, x23]))\n",
    "X3 = np.transpose(np.array([x31, x32, x33]))\n",
    "X = np.vstack((X1, X2, X3))\n",
    "\n",
    "Y = np.hstack((y1, y2, y3))\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_neurons_layer = [3,3]\n",
    "\n",
    "def weight_init(input_matrix, no_neurons_layer):\n",
    "    \n",
    "    weight_matrix = list()\n",
    "    no_features = input_matrix.shape[1]\n",
    "    for neurons in no_neurons_layer:\n",
    "        weight_matrix.append(np.random.rand(no_features + 1,neurons))\n",
    "        no_features = neurons\n",
    "    \n",
    "    output_classes = len(np.unique(Y))\n",
    "    weight_matrix.append(np.random.rand(no_features + 1, output_classes) )\n",
    "    return weight_matrix\n",
    "\n",
    "weight_matrix = weight_init(X, no_neurons_layer)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [None] * (len(no_neurons_layer) + 2)\n",
    "learning_rate = 0.001\n",
    "\n",
    "predicted = None\n",
    "grad_hiddenlayer = [None] * len(layers)\n",
    "grad_activation = [None] * len(layers)\n",
    "softmax_layer = len(layers) - 1\n",
    "\n",
    "hidden_layers= [l for l in range(1,len(layers)-1)]\n",
    "\n",
    "def add_bias(inp):\n",
    "    return np.c_[inp,np.ones(inp.shape[0])]\n",
    "\n",
    "def mat_multiply(mat1, mat2):\n",
    "    return np.matmul(mat1, mat2)\n",
    "\n",
    "def sigmoid(inp):\n",
    "    return (1/(1 + np.exp(inp)))\n",
    "\n",
    "def softmax(inp):\n",
    "    op_exp = np.exp(inp)\n",
    "    return op_exp/op_exp.sum(axis = 1)[:,None]\n",
    "\n",
    "def output_class_encode(inp):\n",
    "    inp = inp.astype(int)\n",
    "    classes = np.unique(inp)\n",
    "    no_classes = classes.shape[0]\n",
    "    no_dim = inp.shape[0]\n",
    "    zero_arr = np.zeros((no_dim, no_classes))\n",
    "    zero_arr[np.arange(no_dim), inp] = 1.0\n",
    "    return zero_arr\n",
    "\n",
    "def least_squares_error(actual, predicted):\n",
    "    return 0.5 * np.sum(np.square(np.subtract(predicted,actual)), axis = 0) * (1/actual.shape[0])\n",
    "\n",
    "def cross_entropy(actual, predicted):\n",
    "    return np.sum(np.multiply(actual, np.log(predicted)) * -1)\n",
    "\n",
    "def derivative_error_softmax_output(actual, predicted):\n",
    "    return np.multiply((1/predicted),actual) * -1\n",
    "    \n",
    "def derivative_loss_output_activation(actual, predicted): #d(Loss)/d(softmax)\n",
    "    grad_softmax = (actual - predicted) * -1\n",
    "    grad_activation[softmax_layer] = [None] * grad_softmax.shape[1]\n",
    "    for i in range(grad_softmax.shape[1]):\n",
    "        grad_activation[softmax_layer][i] = grad_softmax[:,i]\n",
    "\n",
    "def derivative_sigmoid(): #d(sigmoid/d(activation_input)) or dh/da\n",
    "    global grad_sigmoid\n",
    "    \n",
    "    grad_sigmoid = [None] * len(layers)  \n",
    "    \n",
    "    for i in range(1,len(layers)-1):\n",
    "        grad_sigmoid[i] = [None] * len(layers[i])  \n",
    "        for j in range(layers[i].shape[1]):\n",
    "            grad_sigmoid[i][j] = layers[i][:,j] * (1 - layers[i][:,j])\n",
    "        \n",
    "\n",
    "def derivative_activation_previous_activation(grad_softmax, layers): #d(Loss)/d(hidden_l)\n",
    "           \n",
    "    for i in sorted(hidden_layers,reverse = True):\n",
    "        grad_hiddenlayer[i] = [None] * layers[i].shape[1]\n",
    "        grad_activation[i] = [None] * layers[i].shape[1]\n",
    "        for j in range(layers[i].shape[1]):\n",
    "            grad_hiddenlayer[i][j] = [np.zeros(layers[i].shape[0])] * layers[i].shape[1]\n",
    "            for m in range(layers[i+1].shape[1]):\n",
    "                grad_hiddenlayer[i][j] += grad_activation[i+1][m] * weight_matrix[i][m][j]\n",
    "                \n",
    "            grad_activation[i][j] = np.multiply(grad_hiddenlayer[i][j],grad_sigmoid[i][j])\n",
    "            \n",
    "\n",
    "def feed_forward(input_matrix, ouptut_class, no_neurons_layer):\n",
    "    \n",
    "    layers[0] = input_matrix #input layer\n",
    "\n",
    "    for layer in range(len(no_neurons_layer) + 1):\n",
    "        layers[layer + 1] = sigmoid(mat_multiply(add_bias(layers[layer]), weight_matrix[layer]))\n",
    "    predicted_op = softmax(layers[len(no_neurons_layer)+1])\n",
    "    actual_op = output_class_encode(ouptut_class)\n",
    "    \n",
    "    global predicted, actual\n",
    "    predicted = predicted_op\n",
    "    actual = actual_op\n",
    "    \n",
    "    return cross_entropy(actual_op,predicted_op)\n",
    "\n",
    "tst = feed_forward(X, Y, no_neurons_layer)\n",
    "\n",
    "\n",
    "def backpropogation():\n",
    "    derivative_loss_output_activation(actual, predicted) #d(Loss)/d(softmax) or dL/dal\n",
    "    derivative_sigmoid() #d(sigmoid/d(activation_input)) or dh/da\n",
    "    \n",
    "    derivative_activation_previous_activation(grad_softmax, layers) #d(Loss)/d(hidden_l) or dL/dhl\n",
    "    \n",
    "backpropogation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-290-ff21bff4bbbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrad_activation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_activation[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
