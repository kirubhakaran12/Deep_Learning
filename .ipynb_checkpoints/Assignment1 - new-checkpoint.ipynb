{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    os.chdir('C:\\\\Users\\\\s106961\\\\Desktop\\\\Deep Learning\\\\Data')\n",
    "    with h5py.File('train_128.h5','r') as H:\n",
    "        data = np.copy(H['data'])\n",
    "    with h5py.File('train_label.h5','r') as H:\n",
    "        label = np.copy(H['label'])\n",
    "    mu = data.mean()\n",
    "    sigma = data.std()\n",
    "    data = (data - mu) / sigma\n",
    "\n",
    "    return data, label\n",
    "\n",
    "data, label = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(X,y,iteration,batch_size = 100, shuffle = False):\n",
    "    dim = len(y)\n",
    "    idx = np.array(range(0,dim)).astype(np.int)\n",
    "    if shuffle == True: \n",
    "        np.random.shuffle(idx)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "    else:\n",
    "        start = (iteration * batch_size) % dim\n",
    "        end = start + batch_size\n",
    "        \n",
    "    X = X[idx[start:end]]\n",
    "    y = y[idx[start:end]]\n",
    "    return X, y\n",
    "\n",
    "def weight_init(X, y, hidden_units):\n",
    "    \n",
    "    Weights = {}\n",
    "    Bias = {}\n",
    "    no_hidden = len(hidden_units)\n",
    "    no_features = X.shape[1]\n",
    "    for n, units in enumerate(hidden_units):\n",
    "        Weights['layer_{}'.format(n+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)),high=np.sqrt(2. / (no_features)),size=(no_features, units))\n",
    "        Bias['layer_{}'.format(n+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)),high=np.sqrt(2. / (no_features)),size=(units))\n",
    "        no_features = units\n",
    "    \n",
    "    labels = len(np.unique(y))\n",
    "    Weights['layer_{}'.format(no_hidden+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)), high=np.sqrt(2. / (no_features)),size=(no_features, labels))\n",
    "    Bias['layer_{}'.format(no_hidden+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)),high=np.sqrt(2. / (no_features)),size=(labels))\n",
    "    \n",
    "    return Weights, Bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(inp):\n",
    "    return (1/(1 + np.exp(-inp)))\n",
    "\n",
    "def tanh(inp):\n",
    "    return (np.exp(inp) - np.exp(-inp))/(np.exp(inp) + np.exp(-inp))\n",
    "\n",
    "def relu(inp):\n",
    "    return np.maximum(inp,0)\n",
    "\n",
    "def softmax(inp):\n",
    "    #op_exp = np.exp(inp)\n",
    "    #return op_exp/op_exp.sum(axis = 1)[:,None]\n",
    "    \n",
    "    #the above code causes overflow. The below one is normalised.\n",
    "    inp_max = inp.max()\n",
    "    inp_norm = inp - inp_max\n",
    "    return np.exp(inp_norm) / np.sum(np.exp(inp_norm), axis=1) [:,None]\n",
    "\n",
    "def leaky_relu(inp):\n",
    "    return np.maximum(inp,0.1*inp)  \n",
    "   \n",
    "activation_fn = {'sigmoid': sigmoid, 'tanh': tanh, 'relu': relu, 'leaky_relu': leaky_relu}\n",
    "\n",
    "def gradient_sigmoid(inp):\n",
    "    return sigmoid(inp) * (1 - sigmoid(inp))\n",
    "\n",
    "def gradient_tanh(inp):\n",
    "    return (1 -tanh(inp)) * (1 + tanh(inp))\n",
    "\n",
    "def gradient_relu(inp):\n",
    "    inp[inp>0] = 1\n",
    "    inp[inp<0] = 0\n",
    "    return inp\n",
    "\n",
    "def gradient_leaky_relu(inp):\n",
    "    inp[inp<0] = 0.1\n",
    "    inp[inp>=0] = 1\n",
    "    return inp\n",
    "    \n",
    "gradient_fn = {'sigmoid': gradient_sigmoid, 'tanh': gradient_tanh, 'relu': gradient_relu, 'leaky_relu':gradient_leaky_relu}\n",
    "\n",
    "def output_label_hot_encode_old(inp):\n",
    "    inp = inp.astype(int)\n",
    "    classes = np.unique(inp)\n",
    "    no_classes = classes.shape[0]\n",
    "    no_dim = inp.shape[0]\n",
    "    zero_arr = np.zeros((no_dim, no_classes))\n",
    "    zero_arr[np.arange(no_dim), inp] = 1.0\n",
    "    return zero_arr\n",
    "\n",
    "def output_label_hot_encode(inp,batch_size,no_label):\n",
    "    one_hot_array = np.zeros((batch_size,no_label))\n",
    "    one_hot_array[np.arange(batch_size),inp] = 1\n",
    "    return one_hot_array\n",
    "\n",
    "def cross_entropy_error(y_act, y_hat):\n",
    "    return np.sum(np.multiply(y_act, np.log(y_hat)) * -1)\n",
    "\n",
    "def accuracy(y_hat, y_act):\n",
    "    return round((np.sum((y_hat.argmax(axis=1) == y_act.argmax(axis=1)))/y_act.shape[0] * 100),2)\n",
    "\n",
    "def predict(data, label, Weights, Bias, no_hidden = 2, activation = 'relu'):\n",
    "    layers, y_hat = forward_propagation(data, label, Weights, Bias, no_hidden, activation)\n",
    "    y_act = output_label_hot_encode(label, data.shape[0], no_label = 10)\n",
    "    return y_hat, y_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, y, Weights, Bias, no_hidden, activation):\n",
    "    \n",
    "    layers = {}\n",
    "    layers['layer_{}'.format(0)] = X #input layer\n",
    "\n",
    "    for l in range(no_hidden):\n",
    "        layers['layer_wx_{}'.format(l+1)] = np.matmul(layers['layer_{}'.format(l)], Weights['layer_{}'.format(l+1)]) + Bias['layer_{}'.format(l+1)]\n",
    "        layers['layer_{}'.format(l+1)] = activation_fn[activation](layers['layer_wx_{}'.format(l+1)])\n",
    "        \n",
    "    layers['layer_wx_{}'.format(no_hidden+1)] = np.matmul(layers['layer_{}'.format(no_hidden)], Weights['layer_{}'.format(no_hidden+1)])\n",
    "    layers['layer_{}'.format(no_hidden+1)] = softmax(layers['layer_wx_{}'.format(no_hidden+1)])\n",
    "    \n",
    "    y_hat = layers['layer_{}'.format(no_hidden+1)]\n",
    "        \n",
    "    return (layers, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(layers, y_act, y_hat, Weights, Bias, no_hidden, activation):\n",
    "    \n",
    "    gradients = {}\n",
    "    dim = y_act.shape[0]\n",
    "    \n",
    "    gradients['dL/dA_layer_{}'.format(no_hidden+1)]  = ((y_act - y_hat) * -1)/dim\n",
    "    \n",
    "    for i in range(no_hidden,-1,-1):\n",
    "        \n",
    "        gradients['dL/dW_layer{}'.format(i+1)] = np.zeros(Weights['layer_{}'.format(i+1)].shape)\n",
    "        for j in range(Weights['layer_{}'.format(i+1)].shape[0]):\n",
    "            for k in range(Weights['layer_{}'.format(i+1)].shape[1]):\n",
    "                gradients['dL/dW_layer{}'.format(i+1)][j][k] = np.sum((np.multiply(gradients['dL/dA_layer_{}'.format(i+1)][:,k], layers['layer_{}'.format(i)][:,j])))\n",
    "                \n",
    "        gradients['dL/dB_layer{}'.format(i+1)] = np.sum(gradients['dL/dA_layer_{}'.format(i+1)],axis=0)\n",
    "        if i != 0:\n",
    "            gradients['dL/dH_layer{}'.format(i)] = np.matmul(gradients['dL/dA_layer_{}'.format(i+1)],np.transpose(Weights['layer_{}'.format(i+1)]))\n",
    "            gradients['dL/dA_layer_{}'.format(i)] = gradient_fn[activation](layers['layer_wx_{}'.format(i)]) * gradients['dL/dH_layer{}'.format(i)]\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(gradients, Weights, Bias, learning_rate,no_hidden):\n",
    "    for i in range(no_hidden+1,0,-1):\n",
    "        Weights['layer_{}'.format(i)] = Weights['layer_{}'.format(i)] - (learning_rate * gradients['dL/dW_layer{}'.format(i)])\n",
    "        Bias['layer_{}'.format(i)] = Bias['layer_{}'.format(i)] - (learning_rate * gradients['dL/dB_layer{}'.format(i)])\n",
    "        \n",
    "    return Weights, Bias\n",
    "\n",
    "def momentum_gradient_descent(gradients, Weights, Bias, learning_rate,no_hidden, rho, prev_gradients):\n",
    "    momentum_gradient = {}\n",
    "    for i in range(no_hidden+1,0,-1):\n",
    "        momentum_gradient['dL/dW_layer{}'.format(i)] = (rho * prev_gradients['dL/dW_layer{}'.format(i)]) + (learning_rate * gradients['dL/dW_layer{}'.format(i)])\n",
    "        momentum_gradient['dL/dB_layer{}'.format(i)] = (rho * prev_gradients['dL/dB_layer{}'.format(i)]) + (learning_rate * gradients['dL/dB_layer{}'.format(i)])\n",
    "        Weights['layer_{}'.format(i)] = Weights['layer_{}'.format(i)] - momentum_gradient['dL/dW_layer{}'.format(i)]\n",
    "        Bias['layer_{}'.format(i)] = Bias['layer_{}'.format(i)] - momentum_gradient['dL/dB_layer{}'.format(i)]\n",
    "        \n",
    "    return Weights, Bias, momentum_gradient\n",
    "\n",
    "def adam(gradients, Weights, Bias, learning_rate,no_hidden, prev_gradients, prev_velocity, beta1, beta2, time):\n",
    "    momentum_gradient = {}\n",
    "    velocity = {}\n",
    "    ephsilon = 1e-5\n",
    "    for i in range(no_hidden+1,0,-1):\n",
    "        momentum_gradient['dL/dW_layer{}'.format(i)] = ((beta1 * prev_gradients['dL/dW_layer{}'.format(i)]) + ((1-beta1) * gradients['dL/dW_layer{}'.format(i)]))#/(1-beta1**time)\n",
    "        momentum_gradient['dL/dB_layer{}'.format(i)] = ((beta1 * prev_gradients['dL/dB_layer{}'.format(i)]) + ((1-beta1) * gradients['dL/dB_layer{}'.format(i)]))#/(1-beta1**time)\n",
    "        velocity['dL/dW_layer{}'.format(i)] = (beta2 * prev_velocity['dL/dW_layer{}'.format(i)]) + ((1-beta2) * np.square(gradients['dL/dW_layer{}'.format(i)]))#/(1-beta2**time)\n",
    "        velocity['dL/dB_layer{}'.format(i)] = (beta2 * prev_velocity['dL/dB_layer{}'.format(i)]) + ((1-beta2) * np.square(gradients['dL/dB_layer{}'.format(i)]))#/(1-beta2**time)\n",
    "        \n",
    "        Weights['layer_{}'.format(i)] = Weights['layer_{}'.format(i)] - ((learning_rate * momentum_gradient['dL/dW_layer{}'.format(i)])/(np.sqrt(velocity['dL/dW_layer{}'.format(i)]) + ephsilon))\n",
    "        Bias['layer_{}'.format(i)] = Bias['layer_{}'.format(i)] - ((learning_rate * momentum_gradient['dL/dB_layer{}'.format(i)])/(np.sqrt(velocity['dL/dB_layer{}'.format(i)]) + ephsilon))\n",
    "\n",
    "    return Weights, Bias, momentum_gradient, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, label, hidden_units, Weights, Bias, activation = 'sigmoid', descent_method = 'momentum_gradient_descent', \n",
    "          epochs = 10000, learning_rate = 0.01,batch_size = 100):\n",
    "    \n",
    "    start = time.time()    \n",
    "    error = []   \n",
    "    no_hidden = len(hidden_units)\n",
    "    no_label = len(np.unique(label))\n",
    "    rho = 0.9\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "\n",
    "    for i in range(epochs):\n",
    "        X, y = mini_batch(data,label,i,batch_size,shuffle = False)\n",
    "        y_act = output_label_hot_encode(y, batch_size, no_label)\n",
    "        layers, y_hat = forward_propagation(X, y, Weights, Bias, no_hidden, activation)\n",
    "        gradients = backward_propagation(layers, y_act, y_hat, Weights, Bias, no_hidden, activation)\n",
    "        \n",
    "        if descent_method == 'gradient_descent':\n",
    "            Weights, Bias = gradient_descent(gradients, Weights, Bias, learning_rate, no_hidden)\n",
    "        elif descent_method == 'momentum_gradient_descent':\n",
    "            if i == 0:\n",
    "                prev_gradients = {layer:gradients[layer] * 0 for layer in gradients}\n",
    "            Weights, Bias, prev_gradients = momentum_gradient_descent(gradients, Weights, Bias, learning_rate, no_hidden,\n",
    "                                                                      rho, prev_gradients)\n",
    "        elif descent_method == 'adam':\n",
    "            if i == 0:\n",
    "                prev_gradients = {layer:gradients[layer] * 0 for layer in gradients}\n",
    "                prev_velocity = {layer:gradients[layer] * 0 for layer in gradients}\n",
    "            Weights, Bias, prev_gradients, prev_velocity = adam(gradients, Weights, Bias, learning_rate,no_hidden, \n",
    "                                                  prev_gradients, prev_velocity, beta1, beta2, time = i+1)\n",
    "            \n",
    "        ce_error = cross_entropy_error(y_act, y_hat)\n",
    "        \n",
    "        if i%100 == 0: error.append(ce_error)\n",
    "        if i%100 == 0: \n",
    "            pred_y_hat, pred_y_act = predict(data, label, Weights, Bias, no_hidden, activation = 'relu')\n",
    "            print('Epoch:' + str(i) + ' Error:' + str(round(ce_error,2)) + ' Accuracy:' + str(accuracy(pred_y_hat, pred_y_act)))\n",
    "        \n",
    "    end = time.time()\n",
    "    print('Training time: ' + str(round((end - start)/60)) + ' mins')\n",
    "    return error, y_act, y_hat, Weights, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [50,25,10]\n",
    "no_hidden = len(hidden_units)\n",
    "Weights, Bias = weight_init(data, label, hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Error:230.14 Accuracy:15.43\n",
      "Epoch:100 Error:51.16 Accuracy:80.14\n"
     ]
    }
   ],
   "source": [
    "error, y_act, y_hat, Weight_new, iters = train(data, label, hidden_units, Weights, Bias, activation = 'relu',\n",
    "                                               descent_method = 'adam', epochs = 1000, \n",
    "                                               learning_rate = 0.01,batch_size = 100)\n",
    "pred_y_hat, pred_y_act = predict(data, label, Weights, Bias, no_hidden, activation = 'relu')\n",
    "print('Accuracy ' + str(accuracy(pred_y_hat, pred_y_act)))\n",
    "plt.plot(error)\n",
    "plt.ylabel('Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 80.91\n"
     ]
    }
   ],
   "source": [
    "pred_y_hat, pred_y_act = predict(data, label, Weights, Bias, no_hidden, activation = 'relu')\n",
    "print('Accuracy ' + str(accuracy(pred_y_hat, pred_y_act)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
