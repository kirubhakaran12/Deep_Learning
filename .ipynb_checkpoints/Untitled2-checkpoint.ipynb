{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "size = 100\n",
    "x11 = np.random.uniform(low=0.0,high=1.0,size=size)\n",
    "x12 = np.random.uniform(low=2.0,high=8.0,size=size)\n",
    "x13 = np.random.uniform(low=6.0,high=8.0,size=size)\n",
    "\n",
    "x21 = np.random.uniform(low=10.0,high=11.0,size=size)\n",
    "x22 = np.random.uniform(low=12.0,high=18.0,size=size)\n",
    "x23 = np.random.uniform(low=16.0,high=18.0,size=size)\n",
    "\n",
    "x31 = np.random.uniform(low=20.0,high=21.0,size=size)\n",
    "x32 = np.random.uniform(low=22.0,high=28.0,size=size)\n",
    "x33 = np.random.uniform(low=26.0,high=28.0,size=size)\n",
    "\n",
    "y1 = np.transpose(np.zeros(100))\n",
    "y2 = np.transpose(np.ones(100))\n",
    "y3 = np.transpose(np.ones(100) * 2)\n",
    "\n",
    "X1 = np.transpose(np.array([x11, x12, x13]))\n",
    "X2 = np.transpose(np.array([x21, x22, x23]))\n",
    "X3 = np.transpose(np.array([x31, x32, x33]))\n",
    "X = np.matrix((np.vstack((X1, X2, X3)))).T\n",
    "\n",
    "y = np.matrix((np.hstack((y1, y2, y3)))).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "\n",
    "\n",
    "def activation_function(x, activation = 'sigmoid'):\n",
    "    if (activation == 'sigmoid'):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    elif (activation == 'tanh'):\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "    elif activation == 'relu':\n",
    "        x[np.where(x < 0)] = 0.0\n",
    "        return x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def output_function(x, activation = 'softmax'):\n",
    "    if (activation == 'softmax'):\n",
    "        x = np.exp(x - np.max(x, axis = 0))  # Normalization for numerical stability, from CS231n notes\n",
    "        return x / np.sum(x, axis=0)\n",
    "    if (activation == 'sigmoid'):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def loss_function(y_true, y_pred, loss = 'ce'):\n",
    "    batch_size = y_true.shape[0]\n",
    "    if loss == 'sq':\n",
    "        e_y = np.zeros_like(y_pred)\n",
    "        e_y[y_true, range(batch_size)] = 1\n",
    "        return (1.0 / (2.0 * batch_size)) * np.sum((e_y - y_pred)**2)\n",
    "    if loss == 'ce':\n",
    "        return (-1.0 / batch_size) * np.log(y_pred[y_true, range(batch_size)]).sum()\n",
    "\n",
    "\n",
    "def setup_logger(name, log_file, level=logging.INFO):\n",
    "    \"\"\"Function setup as many loggers as you want\"\"\"\n",
    "\n",
    "    handler = logging.FileHandler(log_file, mode = 'w')        \n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, num_hidden, sizes, activation_choice = 'softmax', output_choice = 'softmax', loss_choice = 'ce'):\n",
    "        # L hidden layers, layer 0 is input, layer (L+1) is output\n",
    "        self.sizes = sizes\n",
    "        sizes = [300] + sizes + [3]\n",
    "        self.L = num_hidden\n",
    "        self.output_shape = 10\n",
    "        # Parameter map from theta to Ws and bs\n",
    "        self.param_map = {}\n",
    "        start, end = 0, 0\n",
    "        for i in range(1, self.L + 2):\n",
    "            end = start + sizes[i - 1] * sizes[i]\n",
    "            self.param_map['W{}'.format(i)] = (start, end)\n",
    "            start = end\n",
    "            end = start + sizes[i]\n",
    "            self.param_map['b{}'.format(i)] = (start, end)\n",
    "            start = end\n",
    "        num_params = end\n",
    "        # Parameter vector - theta\n",
    "        self.theta = np.random.uniform(-1.0, 1.0, num_params)\n",
    "        # Gradient vector - theta\n",
    "        self.grad_theta = np.zeros_like(self.theta)\n",
    "        # Map theta (grad_theta) to params (grad_params)\n",
    "        self.params = {}\n",
    "        self.grad_params = {}\n",
    "        for i in range(1, self.L + 2):\n",
    "            weight = 'W{}'.format(i)\n",
    "            start, end = self.param_map[weight]\n",
    "            self.params[weight] = self.theta[start : end].reshape((sizes[i], sizes[i - 1]))\n",
    "            self.grad_params[weight] = self.grad_theta[start : end].reshape((sizes[i], sizes[i - 1]))\n",
    "            bias = 'b{}'.format(i)\n",
    "            start, end = self.param_map[bias]\n",
    "            self.params[bias] = self.theta[start : end].reshape((sizes[i], 1))\n",
    "            self.grad_params[bias] = self.grad_theta[start : end].reshape((sizes[i], 1))\n",
    "\n",
    "        self.activation_choice = activation_choice\n",
    "        self.output_choice = output_choice\n",
    "        self.loss_choice = loss_choice\n",
    "\n",
    "    # x is of shape (input_size, batch_size), y is of shape (batch_size)\n",
    "    def forward(self, x, y):\n",
    "        # a(i) = b(i) + W(i)*h(i-1)\n",
    "        # h(i) = g(i-1)\n",
    "        self.activations = {}\n",
    "        self.activations['h0'] = x\n",
    "        self.batch_size = x.shape[1]\n",
    "        for i in range (1, self.L + 1):\n",
    "            self.activations['a{}'.format(i)] = self.params['b{}'.format(i)] + np.matmul(self.params['W{}'.format(i)], self.activations['h{}'.format(i-1)])\n",
    "            self.activations['h{}'.format(i)] = activation_function(self.activations['a{}'.format(i)], self.activation_choice)\n",
    "\n",
    "        self.activations['a{}'.format(self.L + 1)] = self.params['b{}'.format(self.L + 1)] + np.matmul(self.params['W{}'.format(self.L+1)], self.activations['h{}'.format(self.L)])\n",
    "        y_pred = output_function(self.activations['a{}'.format(self.L + 1)], self.output_choice)\n",
    "        loss = loss_function(y, y_pred, self.loss_choice)\n",
    "        return y_pred, loss\n",
    "\n",
    "    def backward(self, y_true, y_pred):\n",
    "        grad_activations = {}\n",
    "        # Compute output gradient\n",
    "        e_y = np.zeros_like(y_pred)\n",
    "        e_y[y_true, range(self.batch_size)] = 1\n",
    "        if self.loss_choice == 'ce':\n",
    "            grad_activations['a{}'.format(self.L + 1)] = -(e_y - y_pred)\n",
    "        elif self.loss_choice == 'sq':\n",
    "            grad_activations['a{}'.format(self.L + 1)] = -(e_y - y_pred) * y_pred * (1 - y_pred)\n",
    "        for k in range (self.L + 1, 0, -1):\n",
    "            # Gradients wrt parameters\n",
    "            self.grad_params['W{}'.format(k)][:, :] = (1.0 / self.batch_size) * np.matmul(grad_activations['a{}'.format(k)], self.activations['h{}'.format(k-1)].T)\n",
    "            self.grad_params['b{}'.format(k)][:, :] = (1.0 / self.batch_size) * np.sum(grad_activations['a{}'.format(k)], axis = 1, keepdims = True)\n",
    "            # Do not compute gradients with respect to the inputs\n",
    "            if k == 1:\n",
    "                break\n",
    "            # Gradients wrt prev layer\n",
    "            grad_activations['h{}'.format(k-1)] = np.matmul(self.params['W{}'.format(k)].T, grad_activations['a{}'.format(k)])\n",
    "            # Gradients wrt prev preactivation\n",
    "            if self.activation_choice == 'sigmoid':\n",
    "                grad_activation_ = np.multiply(self.activations['h{}'.format(k - 1)], 1 - self.activations['h{}'.format(k - 1)])\n",
    "            elif self.activation_choice == 'tanh':\n",
    "                grad_activation_ = 1 - (self.activations['h{}'.format(k - 1)]) ** 2\n",
    "            elif self.activation_choice == 'relu':\n",
    "                grad_activation_ = np.zeros_like(self.activations['a{}'.format(k - 1)])\n",
    "                grad_activation_[np.where(self.activations['a{}'.format(k - 1)] > 0)] = 1.0\n",
    "            grad_activations['a{}'.format(k-1)] = np.multiply(grad_activations['h{}'.format(k-1)], grad_activation_)\n",
    "\n",
    "    def performance(self, y_true, y_pred):\n",
    "        y_pred = y_pred.argmax(axis = 0)\n",
    "        return float(np.sum(y_pred != y_true)) /y_pred.shape[0] * 100\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred, _ = self.forward(x, np.ones((x.shape[1]), dtype = np.int16))\n",
    "        return y_pred.argmax(axis = 0)\n",
    "\n",
    "    def save(self, path):\n",
    "        np.save(path, self.theta)\n",
    "\n",
    "    def load(self, theta = None, path = None):\n",
    "        if path != None:\n",
    "            theta = np.load(path)\n",
    "        self.theta[:] = theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,300) and (3,300) not aligned: 300 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-e7486ada7714>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_choice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_choice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_choice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-4e69dddcd616>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'a{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'h{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'h{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'a{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_choice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (3,300) and (3,300) not aligned: 300 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "a = Network(3, [3,3,3], activation_choice = 'softmax', output_choice = 'softmax', loss_choice = 'loss')\n",
    "y_pred, loss = a.forward(X,y)\n",
    "a.backward(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, y):\n",
    "    # a(i) = b(i) + W(i)*h(i-1)\n",
    "    # h(i) = g(i-1)\n",
    "    activations = {}\n",
    "    activations['h0'] = x\n",
    "    batch_size = x.shape[1]\n",
    "    activation_choice = 'softmax'\n",
    "    \n",
    "    for i in range (1, self.L + 1):\n",
    "        activations['a{}'.format(i)] = params['b{}'.format(i)] + np.matmul(self.params['W{}'.format(i)], activations['h{}'.format(i-1)])\n",
    "        activations['h{}'.format(i)] = activation_function(activations['a{}'.format(i)], activation_choice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
