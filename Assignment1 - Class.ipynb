{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "def load_data():\n",
    "    os.chdir('C:\\\\Users\\\\s106961\\\\Desktop\\\\Deep Learning\\\\Data')\n",
    "    with h5py.File('train_128.h5','r') as H:\n",
    "        data = np.copy(H['data'])\n",
    "    with h5py.File('train_label.h5','r') as H:\n",
    "        label = np.copy(H['label'])\n",
    "    return data, label\n",
    "\n",
    "data, label = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation functions and their derivatives\n",
    "\n",
    "def sigmoid(inp):\n",
    "    return (1/(1 + np.exp(-inp)))\n",
    "\n",
    "def tanh(inp):\n",
    "    return (np.exp(inp) - np.exp(-inp))/(np.exp(inp) + np.exp(-inp))\n",
    "\n",
    "def relu(inp):\n",
    "    return np.maximum(inp,0)\n",
    "\n",
    "def softmax(inp):\n",
    "    inp_max = inp.max()\n",
    "    inp_norm = inp - inp_max\n",
    "    return np.exp(inp_norm) / np.sum(np.exp(inp_norm), axis=1) [:,None]\n",
    "\n",
    "def leaky_relu(inp):\n",
    "    return np.maximum(inp,0.1*inp)  \n",
    "   \n",
    "def gradient_sigmoid(inp):\n",
    "    return sigmoid(inp) * (1 - sigmoid(inp))\n",
    "\n",
    "def gradient_tanh(inp):\n",
    "    return (1 -tanh(inp)) * (1 + tanh(inp))\n",
    "\n",
    "def gradient_relu(inp):\n",
    "    inp[inp>0] = 1\n",
    "    inp[inp<0] = 0\n",
    "    return inp\n",
    "\n",
    "def gradient_leaky_relu(inp):\n",
    "    inp[inp<0] = 0.1\n",
    "    inp[inp>=0] = 1\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self,data,label,hidden_units=[50,25],activation='relu',descent_method='momentum_gradient_descent',\n",
    "                 learning_rate = 0.01,batch_size = 100,shuffle = False, regularization = 0.001,\n",
    "                 dropout_input_layer_ratio = 0.8, dropout_hidden_layer_ratio = 0.5):\n",
    "        \n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.mu = data.mean()\n",
    "        self.sigma = data.std()\n",
    "        self.X = self.batch_normalize(data)\n",
    "        self.y = label\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.descent_method = descent_method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.no_hidden = len(hidden_units)\n",
    "        self.no_features = data.shape[1]\n",
    "        self.no_label = len(np.unique(label))\n",
    "        self.dim = len(label)\n",
    "        self.shuffle = shuffle\n",
    "        self.Weights, self.Bias = self.weight_init()\n",
    "        self.regularization = regularization\n",
    "        self.dropout_input_layer_ratio = dropout_input_layer_ratio\n",
    "        self.dropout_hidden_layer_ratio = dropout_hidden_layer_ratio\n",
    "        self.activation_fn = {'sigmoid': sigmoid, 'tanh': tanh, 'relu': relu, 'leaky_relu': leaky_relu}\n",
    "        self.gradient_fn = {'sigmoid': gradient_sigmoid, 'tanh': gradient_tanh, 'relu': gradient_relu, 'leaky_relu':gradient_leaky_relu}\n",
    "        self.error = []\n",
    "    \n",
    "    def batch_normalize(self, data):\n",
    "        data = (data - self.mu) / self.sigma\n",
    "        return data\n",
    "    \n",
    "    def weight_init(self):\n",
    "    \n",
    "        Weights = {}\n",
    "        Bias = {}\n",
    "        no_hidden = self.no_hidden\n",
    "        no_features = self.no_features\n",
    "        labels = self.no_label\n",
    "        \n",
    "        for n, units in enumerate(self.hidden_units):\n",
    "            Weights['layer_{}'.format(n+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)),high=np.sqrt(2. / (no_features)),size=(no_features, units))\n",
    "            Bias['layer_{}'.format(n+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)),high=np.sqrt(2. / (no_features)),size=(units))\n",
    "            no_features = units\n",
    "\n",
    "        Weights['layer_{}'.format(no_hidden+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)), high=np.sqrt(2. / (no_features)),size=(no_features, labels))\n",
    "        Bias['layer_{}'.format(no_hidden+1)] = np.random.uniform(low=-np.sqrt(2. / (no_features)),high=np.sqrt(2. / (no_features)),size=(labels))\n",
    "\n",
    "        return Weights, Bias \n",
    "    \n",
    "    def mini_batch(self,iteration):\n",
    "        \n",
    "        idx = np.array(range(0,self.dim)).astype(np.int)\n",
    "        if self.shuffle == True: \n",
    "            np.random.shuffle(idx)\n",
    "            start = 0\n",
    "            end = batch_size\n",
    "        else:\n",
    "            start = (iteration * self.batch_size) % self.dim\n",
    "            end = start + self.batch_size\n",
    "\n",
    "        X = self.X[idx[start:end]]\n",
    "        y = self.y[idx[start:end]]\n",
    "        return X, y\n",
    "    \n",
    "    def output_label_hot_encode(self,inp,batch_size):\n",
    "        \n",
    "        one_hot_array = np.zeros((batch_size,self.no_label))\n",
    "        one_hot_array[np.arange(batch_size),inp] = 1\n",
    "        return one_hot_array\n",
    "        \n",
    "    def accuracy(self,y_hat, y_act):\n",
    "        \n",
    "        return round((np.sum((y_hat.argmax(axis=1) == y_act.argmax(axis=1)))/y_act.shape[0] * 100),2)\n",
    "    \n",
    "    def cross_entropy_error(self, y_act, y_hat):\n",
    "        \n",
    "        Weights = self.Weights\n",
    "        regularization = self.regularization\n",
    "        \n",
    "        regularized_loss = 0\n",
    "        for layer in Weights:\n",
    "            regularized_loss += np.sum(np.square(Weights[layer]))\n",
    "        return np.sum(np.multiply(y_act, np.log(y_hat)) * -1) + (0.5 * regularization  * regularized_loss)\n",
    "    \n",
    "    def forward_propagation(self, X, y, dropout_units, predict = False):\n",
    "        \n",
    "        Weights = self.Weights\n",
    "        Bias = self.Bias\n",
    "        no_hidden = self.no_hidden\n",
    "        activation = self.activation\n",
    "        \n",
    "        layers = {}\n",
    "        layers['layer_{}'.format(0)] = X #input layer\n",
    "        if predict == False:\n",
    "            layers['layer_{}'.format(0)][:,dropout_units['layer_{}'.format(0)]] = 0\n",
    "\n",
    "        for l in range(no_hidden):\n",
    "            layers['layer_wx_{}'.format(l+1)] = np.matmul(layers['layer_{}'.format(l)], Weights['layer_{}'.format(l+1)]) + Bias['layer_{}'.format(l+1)]\n",
    "            layers['layer_{}'.format(l+1)] = self.activation_fn[activation](layers['layer_wx_{}'.format(l+1)])\n",
    "\n",
    "            if predict == False:\n",
    "                layers['layer_wx_{}'.format(l+1)][:,dropout_units['layer_{}'.format(l+1)]] = 0\n",
    "                layers['layer_{}'.format(l+1)][:,dropout_units['layer_{}'.format(l+1)]] = 0\n",
    "\n",
    "        layers['layer_wx_{}'.format(no_hidden+1)] = np.matmul(layers['layer_{}'.format(no_hidden)], Weights['layer_{}'.format(no_hidden+1)])\n",
    "        layers['layer_{}'.format(no_hidden+1)] = softmax(layers['layer_wx_{}'.format(no_hidden+1)])\n",
    "\n",
    "        y_hat = layers['layer_{}'.format(no_hidden+1)]\n",
    "\n",
    "        return (layers, y_hat)\n",
    "    \n",
    "    def backward_propagation(self, layers, y_act, y_hat):\n",
    "    \n",
    "        Weights = self.Weights\n",
    "        Bias = self.Bias\n",
    "        no_hidden = self.no_hidden\n",
    "        activation = self.activation\n",
    "        regularization = self.regularization\n",
    "        \n",
    "        gradients = {}\n",
    "        dim = y_act.shape[0]\n",
    "\n",
    "        gradients['dL/dA_layer_{}'.format(no_hidden+1)]  = ((y_act - y_hat) * -1)/dim\n",
    "\n",
    "        for i in range(no_hidden,-1,-1):\n",
    "\n",
    "            gradients['dL/dW_layer{}'.format(i+1)] = np.zeros(Weights['layer_{}'.format(i+1)].shape)\n",
    "            for j in range(Weights['layer_{}'.format(i+1)].shape[0]):\n",
    "                for k in range(Weights['layer_{}'.format(i+1)].shape[1]):\n",
    "                    gradients['dL/dW_layer{}'.format(i+1)][j][k] = np.sum((np.multiply(gradients['dL/dA_layer_{}'.format(i+1)][:,k], layers['layer_{}'.format(i)][:,j]))) + (regularization * Weights['layer_{}'.format(i+1)][j][k])\n",
    "\n",
    "            gradients['dL/dB_layer{}'.format(i+1)] = np.sum(gradients['dL/dA_layer_{}'.format(i+1)],axis=0)\n",
    "            if i != 0:\n",
    "                gradients['dL/dH_layer{}'.format(i)] = np.matmul(gradients['dL/dA_layer_{}'.format(i+1)],np.transpose(Weights['layer_{}'.format(i+1)]))\n",
    "                gradients['dL/dA_layer_{}'.format(i)] = self.gradient_fn[activation](layers['layer_wx_{}'.format(i)]) * gradients['dL/dH_layer{}'.format(i)]\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def predict(self, data):\n",
    "        \n",
    "        data = self.batch_normalize(data)\n",
    "        layers, y_hat = self.forward_propagation(self.X, self.y, dropout_units = None, predict =True)\n",
    "        y_act = self.output_label_hot_encode(self.y, self.dim)\n",
    "        return y_hat, y_act\n",
    "    \n",
    "    def gradient_descent(self, gradients):\n",
    "        \n",
    "        Weights = self.Weights\n",
    "        Bias = self.Bias\n",
    "        learning_rate = self.learning_rate\n",
    "        no_hidden = self.no_hidden\n",
    "        \n",
    "        for i in range(no_hidden+1,0,-1):\n",
    "            Weights['layer_{}'.format(i)] = Weights['layer_{}'.format(i)] - (learning_rate * gradients['dL/dW_layer{}'.format(i)])\n",
    "            Bias['layer_{}'.format(i)] = Bias['layer_{}'.format(i)] - (learning_rate * gradients['dL/dB_layer{}'.format(i)])\n",
    "\n",
    "        return Weights, Bias\n",
    "    \n",
    "    def momentum_gradient_descent(self, gradients, prev_gradients, rho = 0.9 ):\n",
    "        \n",
    "        Weights = self.Weights\n",
    "        Bias = self.Bias\n",
    "        learning_rate = self.learning_rate\n",
    "        no_hidden = self.no_hidden\n",
    "        \n",
    "        momentum_gradient = {}\n",
    "        for i in range(no_hidden+1,0,-1):\n",
    "            momentum_gradient['dL/dW_layer{}'.format(i)] = (rho * prev_gradients['dL/dW_layer{}'.format(i)]) + (learning_rate * gradients['dL/dW_layer{}'.format(i)])\n",
    "            momentum_gradient['dL/dB_layer{}'.format(i)] = (rho * prev_gradients['dL/dB_layer{}'.format(i)]) + (learning_rate * gradients['dL/dB_layer{}'.format(i)])\n",
    "            Weights['layer_{}'.format(i)] = Weights['layer_{}'.format(i)] - momentum_gradient['dL/dW_layer{}'.format(i)]\n",
    "            Bias['layer_{}'.format(i)] = Bias['layer_{}'.format(i)] - momentum_gradient['dL/dB_layer{}'.format(i)]\n",
    "\n",
    "        return Weights, Bias, momentum_gradient\n",
    "    \n",
    "    def adam(self, gradients, prev_gradients, prev_velocity, beta1, beta2, time):\n",
    "        \n",
    "        momentum_gradient = {}\n",
    "        velocity = {}\n",
    "        ephsilon = 1e-5\n",
    "        \n",
    "        Weights = self.Weights\n",
    "        Bias = self.Bias\n",
    "        learning_rate = self.learning_rate\n",
    "        no_hidden = self.no_hidden\n",
    "        \n",
    "        for i in range(no_hidden+1,0,-1):\n",
    "            momentum_gradient['dL/dW_layer{}'.format(i)] = ((beta1 * prev_gradients['dL/dW_layer{}'.format(i)]) + ((1-beta1) * gradients['dL/dW_layer{}'.format(i)]))#/(1-beta1**time)\n",
    "            momentum_gradient['dL/dB_layer{}'.format(i)] = ((beta1 * prev_gradients['dL/dB_layer{}'.format(i)]) + ((1-beta1) * gradients['dL/dB_layer{}'.format(i)]))#/(1-beta1**time)\n",
    "            velocity['dL/dW_layer{}'.format(i)] = (beta2 * prev_velocity['dL/dW_layer{}'.format(i)]) + ((1-beta2) * np.square(gradients['dL/dW_layer{}'.format(i)]))#/(1-beta2**time)\n",
    "            velocity['dL/dB_layer{}'.format(i)] = (beta2 * prev_velocity['dL/dB_layer{}'.format(i)]) + ((1-beta2) * np.square(gradients['dL/dB_layer{}'.format(i)]))#/(1-beta2**time)\n",
    "\n",
    "            Weights['layer_{}'.format(i)] = Weights['layer_{}'.format(i)] - ((learning_rate * momentum_gradient['dL/dW_layer{}'.format(i)])/(np.sqrt(velocity['dL/dW_layer{}'.format(i)]) + ephsilon))\n",
    "            Bias['layer_{}'.format(i)] = Bias['layer_{}'.format(i)] - ((learning_rate * momentum_gradient['dL/dB_layer{}'.format(i)])/(np.sqrt(velocity['dL/dB_layer{}'.format(i)]) + ephsilon))\n",
    "\n",
    "        return Weights, Bias, momentum_gradient, velocity\n",
    "    \n",
    "    def dropout(self):\n",
    "        \n",
    "        dropout_units = {}\n",
    "        drop_units = np.arange(self.no_features)\n",
    "        np.random.shuffle(drop_units)\n",
    "        dropout_units['layer_{}'.format(0)] = drop_units[:int(self.no_features * (1-self.dropout_input_layer_ratio))]\n",
    "\n",
    "        for (i, units) in enumerate(self.hidden_units,1):\n",
    "            drop_units = np.arange(units)\n",
    "            np.random.shuffle(drop_units)\n",
    "            dropout_units['layer_{}'.format(i)] = drop_units[:int(units * (1-self.dropout_hidden_layer_ratio))]\n",
    "        return dropout_units\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        \n",
    "        start = time.time()             \n",
    "\n",
    "        for i in range(epochs):\n",
    "            dropout_units = self.dropout()\n",
    "            X, y = self.mini_batch(iteration = i)\n",
    "            y_act = self.output_label_hot_encode(y, self.batch_size)\n",
    "            layers, y_hat = self.forward_propagation(X, y, dropout_units)\n",
    "            gradients = self.backward_propagation(layers, y_act, y_hat)\n",
    "\n",
    "            if self.descent_method == 'gradient_descent':\n",
    "                self.Weights, self.Bias = self.gradient_descent(gradients)\n",
    "            elif self.descent_method == 'momentum_gradient_descent':\n",
    "                if i == 0:\n",
    "                    prev_gradients = {layer:gradients[layer] * 0 for layer in gradients}\n",
    "                self.Weights, self.Bias, prev_gradients = self.momentum_gradient_descent(gradients, prev_gradients, rho = 0.9)\n",
    "            elif self.descent_method == 'adam':\n",
    "                if i == 0:\n",
    "                    prev_gradients = {layer:gradients[layer] * 0 for layer in gradients}\n",
    "                    prev_velocity = {layer:gradients[layer] * 0 for layer in gradients}\n",
    "                self.Weights, self.Bias, prev_gradients, prev_velocity = self.adam(gradients, prev_gradients, prev_velocity, beta1 = 0.9, beta2 = 0.999, time = i+1)\n",
    "\n",
    "            ce_error = self.cross_entropy_error(y_act, y_hat)\n",
    "\n",
    "            pred_y_hat, pred_y_act = self.predict(self.data)\n",
    "            curr_accuracy = self.accuracy(pred_y_hat, pred_y_act)\n",
    "            self.error.append(ce_error)\n",
    "            \n",
    "            if i%100 == 0: \n",
    "                print('Epoch:' + str(i) + ' Error:' + str(round(ce_error,2)) + ' Accuracy:' + str(curr_accuracy))\n",
    "\n",
    "        end = time.time()\n",
    "        print('Training time: ' + str(round((end - start)/60)) + ' mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Error:233.23 Accuracy:14.93\n",
      "Epoch:100 Error:142.33 Accuracy:69.8\n",
      "Epoch:200 Error:121.34 Accuracy:73.24\n",
      "Epoch:300 Error:125.86 Accuracy:77.33\n",
      "Epoch:400 Error:164.13 Accuracy:78.74\n",
      "Epoch:500 Error:113.12 Accuracy:78.77\n",
      "Epoch:600 Error:116.77 Accuracy:78.98\n",
      "Epoch:700 Error:112.44 Accuracy:79.99\n",
      "Epoch:800 Error:89.55 Accuracy:80.43\n",
      "Epoch:900 Error:112.79 Accuracy:79.58\n",
      "Training time: 6 mins\n"
     ]
    }
   ],
   "source": [
    "model_1 = MLP(data,label,hidden_units=[50,25],activation='relu',descent_method='adam',\n",
    "              learning_rate = 0.01,batch_size = 100,shuffle = False,regularization = 0.001,\n",
    "              dropout_input_layer_ratio = 0.8, dropout_hidden_layer_ratio = 0.5)\n",
    "model_1.train(epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Error:234.14 Accuracy:10.0\n",
      "Epoch:100 Error:220.37 Accuracy:44.31\n",
      "Epoch:200 Error:201.15 Accuracy:44.67\n",
      "Epoch:300 Error:181.52 Accuracy:49.02\n",
      "Epoch:400 Error:180.77 Accuracy:50.18\n",
      "Epoch:500 Error:253.93 Accuracy:51.22\n",
      "Epoch:600 Error:211.91 Accuracy:51.94\n",
      "Epoch:700 Error:192.96 Accuracy:47.28\n",
      "Epoch:800 Error:226.83 Accuracy:51.62\n",
      "Epoch:900 Error:235.54 Accuracy:48.89\n",
      "Training time: 4 mins\n"
     ]
    }
   ],
   "source": [
    "model_2 = MLP(data,label,hidden_units=[50,10],activation='sigmoid',descent_method='adam',\n",
    "                 learning_rate = 0.01,batch_size = 100,shuffle = False,regularization = 0.001)\n",
    "model_2.train(epochs = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
