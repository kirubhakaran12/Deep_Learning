{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "size = 100\n",
    "x11 = np.random.uniform(low=0.0,high=1.0,size=size)\n",
    "x12 = np.random.uniform(low=2.0,high=8.0,size=size)\n",
    "x13 = np.random.uniform(low=6.0,high=8.0,size=size)\n",
    "\n",
    "x21 = np.random.uniform(low=10.0,high=11.0,size=size)\n",
    "x22 = np.random.uniform(low=12.0,high=18.0,size=size)\n",
    "x23 = np.random.uniform(low=16.0,high=18.0,size=size)\n",
    "\n",
    "x31 = np.random.uniform(low=20.0,high=21.0,size=size)\n",
    "x32 = np.random.uniform(low=22.0,high=28.0,size=size)\n",
    "x33 = np.random.uniform(low=26.0,high=28.0,size=size)\n",
    "\n",
    "y1 = np.transpose(np.zeros(100))\n",
    "y2 = np.transpose(np.ones(100))\n",
    "y3 = np.transpose(np.ones(100) * 2)\n",
    "\n",
    "X1 = np.transpose(np.array([x11, x12, x13]))\n",
    "X2 = np.transpose(np.array([x21, x22, x23]))\n",
    "X3 = np.transpose(np.array([x31, x32, x33]))\n",
    "X = np.vstack((X1, X2, X3))\n",
    "\n",
    "Y = np.hstack((y1, y2, y3))\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_neurons_layer = [3,3]\n",
    "\n",
    "def weight_init(input_matrix, no_neurons_layer):\n",
    "    \n",
    "    weight_matrix = list()\n",
    "    no_features = input_matrix.shape[1]\n",
    "    for neurons in no_neurons_layer:\n",
    "        weight_matrix.append(np.random.rand(no_features + 1,neurons))\n",
    "        no_features = neurons\n",
    "    \n",
    "    output_classes = len(np.unique(Y))\n",
    "    weight_matrix.append(np.random.rand(no_features + 1, output_classes) )\n",
    "    return weight_matrix\n",
    "\n",
    "weight_matrix = weight_init(X, no_neurons_layer)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [None] * (len(no_neurons_layer) + 2)\n",
    "learning_rate = 0.01\n",
    "\n",
    "error = []\n",
    "predicted = None\n",
    "actual = None\n",
    "grad_hiddenlayer = [None] * len(layers)\n",
    "grad_activation = [None] * len(layers)\n",
    "softmax_layer = len(layers) - 1\n",
    "\n",
    "hidden_layers= [l for l in range(1,len(layers)-1)]\n",
    "\n",
    "def add_bias(inp):\n",
    "    return np.c_[inp,np.ones(inp.shape[0])]\n",
    "\n",
    "def mat_multiply(mat1, mat2):\n",
    "    return np.matmul(mat1, mat2)\n",
    "\n",
    "def sigmoid(inp):\n",
    "    return (1/(1 + np.exp(inp)))\n",
    "\n",
    "def softmax(inp):\n",
    "    op_exp = np.exp(inp)\n",
    "    return op_exp/op_exp.sum(axis = 1)[:,None]\n",
    "\n",
    "def output_class_encode(inp):\n",
    "    inp = inp.astype(int)\n",
    "    classes = np.unique(inp)\n",
    "    no_classes = classes.shape[0]\n",
    "    no_dim = inp.shape[0]\n",
    "    zero_arr = np.zeros((no_dim, no_classes))\n",
    "    zero_arr[np.arange(no_dim), inp] = 1.0\n",
    "    return zero_arr\n",
    "\n",
    "def least_squares_error(actual, predicted):\n",
    "    return 0.5 * np.sum(np.square(np.subtract(predicted,actual)), axis = 0) * (1/actual.shape[0])\n",
    "\n",
    "def cross_entropy(actual, predicted):\n",
    "    return np.sum(np.multiply(actual, np.log(predicted)) * -1)\n",
    "  \n",
    "def derivative_loss_output_activation(): #d(Loss)/d(softmax)\n",
    "    grad_softmax = (actual - predicted) * -1\n",
    "    grad_activation[softmax_layer] = [None] * grad_softmax.shape[1]\n",
    "    for i in range(grad_softmax.shape[1]):\n",
    "        grad_activation[softmax_layer][i] = grad_softmax[:,i]\n",
    "\n",
    "def derivative_sigmoid(): #d(sigmoid/d(activation_input)) or dh/da\n",
    "    global grad_sigmoid\n",
    "    \n",
    "    grad_sigmoid = [None] * len(layers)  \n",
    "    \n",
    "    for i in range(1,len(layers)-1):\n",
    "        grad_sigmoid[i] = [None] * len(layers[i])  \n",
    "        for j in range(layers[i].shape[1]):\n",
    "            grad_sigmoid[i][j] = layers[i][:,j] * (1 - layers[i][:,j])\n",
    "        \n",
    "def derivative_activation_previous_activation(): #d(Loss)/d(hidden_l)\n",
    "           \n",
    "    for i in sorted(hidden_layers,reverse = True):\n",
    "        grad_hiddenlayer[i] = [None] * layers[i].shape[1]\n",
    "        grad_activation[i] = [None] * layers[i].shape[1]\n",
    "        for j in range(layers[i].shape[1]):\n",
    "            grad_hiddenlayer[i][j] = np.zeros(layers[i].shape[0])\n",
    "            for m in range(layers[i+1].shape[1] - 1):#-1 because bias doesn't connect to previous layer activation\n",
    "                grad_hiddenlayer[i][j] += grad_activation[i+1][m] * np.transpose(weight_matrix[i])[m][j]\n",
    "                \n",
    "            grad_activation[i][j] = np.multiply(grad_hiddenlayer[i][j],grad_sigmoid[i][j])\n",
    "\n",
    "def derivative_activation_weight():\n",
    "    \n",
    "    global grad_weights\n",
    "    grad_weights = [None] * len(layers)\n",
    "    \n",
    "    for i in range(1, len(layers)):        \n",
    "        if i == len(layers) -1:\n",
    "            units = range(layers[i].shape[1]) #\n",
    "            grad_weights[i] = [None] * (layers[i].shape[1])\n",
    "        else:\n",
    "            units = range(layers[i].shape[1] -1)\n",
    "            grad_weights[i] = [None] * (layers[i].shape[1] - 1)\n",
    "            \n",
    "        for k in units:\n",
    "            grad_weights[i][k] = [None] * layers[i-1].shape[1]\n",
    "            for j in range(layers[i-1].shape[1]):\n",
    "                grad_weights[i][k][j] = np.sum(np.multiply(layers[i-1][:,j], grad_activation[i][k]))/grad_activation[i][k].shape\n",
    "\n",
    "def update_weight(learning_rate):\n",
    "    for i in range(1,len(grad_weights)):\n",
    "        weight_matrix[i-1] = (weight_matrix[i-1] - (learning_rate * np.array(np.transpose(grad_weights[i]))[0]))\n",
    "    \n",
    "def feed_forward(input_matrix, ouptut_class, no_neurons_layer):\n",
    "    \n",
    "    layers[0] = input_matrix #input layer\n",
    "\n",
    "    for layer in range(len(no_neurons_layer) + 1):\n",
    "        layers[layer] = add_bias(layers[layer])\n",
    "        layers[layer + 1] = sigmoid(mat_multiply(layers[layer], weight_matrix[layer]))\n",
    "    predicted_op = softmax(layers[len(no_neurons_layer)+1])\n",
    "    actual_op = output_class_encode(ouptut_class)\n",
    "    \n",
    "    global predicted, actual\n",
    "    predicted = predicted_op\n",
    "    actual = actual_op\n",
    "    \n",
    "    error.append(cross_entropy(actual_op,predicted_op))\n",
    "\n",
    "def backpropagation():\n",
    "    derivative_loss_output_activation() #d(Loss)/d(softmax) or dL/dal\n",
    "    derivative_sigmoid() #d(sigmoid/d(activation_input)) or dh/da\n",
    "    \n",
    "    derivative_activation_previous_activation() #d(Loss)/d(hidden_l) or dL/dhl\n",
    "    derivative_activation_weight()\n",
    "    update_weight(learning_rate)\n",
    "\n",
    "feed_forward(X, Y, no_neurons_layer)    \n",
    "derivative_loss_output_activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 11001%1000 == 0:\n",
    "    print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    feed_forward(X, Y, no_neurons_layer)    \n",
    "    backpropagation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
