{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "size = 100\n",
    "x11 = np.random.uniform(low=0.0,high=1.0,size=size)\n",
    "x12 = np.random.uniform(low=2.0,high=8.0,size=size)\n",
    "x13 = np.random.uniform(low=6.0,high=8.0,size=size)\n",
    "\n",
    "x21 = np.random.uniform(low=10.0,high=11.0,size=size)\n",
    "x22 = np.random.uniform(low=12.0,high=18.0,size=size)\n",
    "x23 = np.random.uniform(low=16.0,high=18.0,size=size)\n",
    "\n",
    "x31 = np.random.uniform(low=20.0,high=21.0,size=size)\n",
    "x32 = np.random.uniform(low=22.0,high=28.0,size=size)\n",
    "x33 = np.random.uniform(low=26.0,high=28.0,size=size)\n",
    "\n",
    "y1 = np.transpose(np.zeros(100))\n",
    "y2 = np.transpose(np.ones(100))\n",
    "y3 = np.transpose(np.ones(100) * 2)\n",
    "\n",
    "X1 = np.transpose(np.array([x11, x12, x13]))\n",
    "X2 = np.transpose(np.array([x21, x22, x23]))\n",
    "X3 = np.transpose(np.array([x31, x32, x33]))\n",
    "X = np.vstack((X1, X2, X3))\n",
    "\n",
    "Y = np.hstack((y1, y2, y3))\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_neurons_layer = [3,3]\n",
    "\n",
    "def weight_init(input_matrix, no_neurons_layer):\n",
    "    \n",
    "    weight_matrix = list()\n",
    "    no_features = input_matrix.shape[1]\n",
    "    for neurons in no_neurons_layer:\n",
    "        weight_matrix.append(np.random.rand(no_features + 1,neurons))\n",
    "        no_features = neurons\n",
    "    \n",
    "    output_classes = len(np.unique(Y))\n",
    "    weight_matrix.append(np.random.rand(no_features + 1, output_classes) )\n",
    "    return weight_matrix\n",
    "\n",
    "weight_matrix = weight_init(X, no_neurons_layer)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [None] * (len(no_neurons_layer) + 2)\n",
    "learning_rate = 0.01\n",
    "\n",
    "error = []\n",
    "predicted = None\n",
    "actual = None\n",
    "grad_hiddenlayer = [None] * len(layers)\n",
    "grad_activation = [None] * len(layers)\n",
    "softmax_layer = len(layers) - 1\n",
    "\n",
    "hidden_layers= [l for l in range(1,len(layers)-1)]\n",
    "\n",
    "def add_bias(inp):\n",
    "    return np.c_[inp,np.ones(inp.shape[0])]\n",
    "\n",
    "def mat_multiply(mat1, mat2):\n",
    "    return np.matmul(mat1, mat2)\n",
    "\n",
    "def sigmoid(inp):\n",
    "    return (1/(1 + np.exp(inp)))\n",
    "\n",
    "def softmax(inp):\n",
    "    op_exp = np.exp(inp)\n",
    "    return op_exp/op_exp.sum(axis = 1)[:,None]\n",
    "\n",
    "def output_class_encode(inp):\n",
    "    inp = inp.astype(int)\n",
    "    classes = np.unique(inp)\n",
    "    no_classes = classes.shape[0]\n",
    "    no_dim = inp.shape[0]\n",
    "    zero_arr = np.zeros((no_dim, no_classes))\n",
    "    zero_arr[np.arange(no_dim), inp] = 1.0\n",
    "    return zero_arr\n",
    "\n",
    "def least_squares_error(actual, predicted):\n",
    "    return 0.5 * np.sum(np.square(np.subtract(predicted,actual)), axis = 0) * (1/actual.shape[0])\n",
    "\n",
    "def cross_entropy(actual, predicted):\n",
    "    return np.sum(np.multiply(actual, np.log(predicted)) * -1)\n",
    "  \n",
    "def derivative_loss_output_activation(): #d(Loss)/d(softmax)\n",
    "    grad_softmax = (actual - predicted) * -1\n",
    "    grad_activation[softmax_layer] = [None] * grad_softmax.shape[1]\n",
    "    for i in range(grad_softmax.shape[1]):\n",
    "        grad_activation[softmax_layer][i] = grad_softmax[:,i]\n",
    "\n",
    "def derivative_sigmoid(): #d(sigmoid/d(activation_input)) or dh/da\n",
    "    global grad_sigmoid\n",
    "    \n",
    "    grad_sigmoid = [None] * len(layers)  \n",
    "    \n",
    "    for i in range(1,len(layers)-1):\n",
    "        grad_sigmoid[i] = [None] * len(layers[i])  \n",
    "        for j in range(layers[i].shape[1]):\n",
    "            grad_sigmoid[i][j] = layers[i][:,j] * (1 - layers[i][:,j])\n",
    "        \n",
    "def derivative_activation_previous_activation(): #d(Loss)/d(hidden_l)\n",
    "           \n",
    "    for i in sorted(hidden_layers,reverse = True):\n",
    "        grad_hiddenlayer[i] = [None] * layers[i].shape[1]\n",
    "        grad_activation[i] = [None] * layers[i].shape[1]\n",
    "        for j in range(layers[i].shape[1]):\n",
    "            grad_hiddenlayer[i][j] = np.zeros(layers[i].shape[0])\n",
    "            for m in range(layers[i+1].shape[1] - 1):#-1 because bias doesn't connect to previous layer activation\n",
    "                grad_hiddenlayer[i][j] += grad_activation[i+1][m] * np.transpose(weight_matrix[i])[m][j]\n",
    "                \n",
    "            grad_activation[i][j] = np.multiply(grad_hiddenlayer[i][j],grad_sigmoid[i][j])\n",
    "\n",
    "def derivative_activation_weight():\n",
    "    \n",
    "    global grad_weights\n",
    "    grad_weights = [None] * len(layers)\n",
    "    \n",
    "    for i in range(1, len(layers)):        \n",
    "        if i == len(layers) -1:\n",
    "            units = range(layers[i].shape[1]) #\n",
    "            grad_weights[i] = [None] * (layers[i].shape[1])\n",
    "        else:\n",
    "            units = range(layers[i].shape[1] -1)\n",
    "            grad_weights[i] = [None] * (layers[i].shape[1] - 1)\n",
    "            \n",
    "        for k in units:\n",
    "            grad_weights[i][k] = [None] * layers[i-1].shape[1]\n",
    "            for j in range(layers[i-1].shape[1]):\n",
    "                grad_weights[i][k][j] = np.sum(np.multiply(layers[i-1][:,j], grad_activation[i][k]))/grad_activation[i][k].shape\n",
    "\n",
    "def update_weight(learning_rate):\n",
    "    for i in range(1,len(grad_weights)):\n",
    "        weight_matrix[i-1] = (weight_matrix[i-1] - (learning_rate * np.array(np.transpose(grad_weights[i]))[0]))\n",
    "    \n",
    "def feed_forward(input_matrix, ouptut_class, no_neurons_layer):\n",
    "    \n",
    "    layers[0] = input_matrix #input layer\n",
    "\n",
    "    for layer in range(len(no_neurons_layer) + 1):\n",
    "        layers[layer] = add_bias(layers[layer])\n",
    "        layers[layer + 1] = sigmoid(mat_multiply(layers[layer], weight_matrix[layer]))\n",
    "    predicted_op = softmax(layers[len(no_neurons_layer)+1])\n",
    "    actual_op = output_class_encode(ouptut_class)\n",
    "    \n",
    "    global predicted, actual\n",
    "    predicted = predicted_op\n",
    "    actual = actual_op\n",
    "    \n",
    "    error.append(cross_entropy(actual_op,predicted_op))\n",
    "\n",
    "def backpropagation():\n",
    "    derivative_loss_output_activation() #d(Loss)/d(softmax) or dL/dal\n",
    "    derivative_sigmoid() #d(sigmoid/d(activation_input)) or dh/da\n",
    "    \n",
    "    derivative_activation_previous_activation() #d(Loss)/d(hidden_l) or dL/dhl\n",
    "    derivative_activation_weight()\n",
    "    update_weight(learning_rate)\n",
    "\n",
    "feed_forward(X, Y, no_neurons_layer)    \n",
    "derivative_loss_output_activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36373897,  0.3289617 ,  0.30729934],\n",
       "       [ 0.36371952,  0.32897883,  0.30730165],\n",
       "       [ 0.3637226 ,  0.32897427,  0.30730313],\n",
       "       [ 0.3637215 ,  0.32897489,  0.30730362],\n",
       "       [ 0.36372158,  0.32897699,  0.30730143],\n",
       "       [ 0.36372383,  0.32897589,  0.30730028],\n",
       "       [ 0.36372126,  0.32897502,  0.30730372],\n",
       "       [ 0.36372652,  0.3289759 ,  0.30729758],\n",
       "       [ 0.36372336,  0.32897089,  0.30730574],\n",
       "       [ 0.36373238,  0.32896546,  0.30730216],\n",
       "       [ 0.36372481,  0.3289728 ,  0.30730238],\n",
       "       [ 0.3637247 ,  0.32897191,  0.3073034 ],\n",
       "       [ 0.36372051,  0.32897664,  0.30730285],\n",
       "       [ 0.36373555,  0.32896487,  0.30729958],\n",
       "       [ 0.36372379,  0.32897084,  0.30730537],\n",
       "       [ 0.36372304,  0.32897148,  0.30730548],\n",
       "       [ 0.36373206,  0.32896964,  0.3072983 ],\n",
       "       [ 0.36372913,  0.32896752,  0.30730335],\n",
       "       [ 0.36372168,  0.32897423,  0.30730408],\n",
       "       [ 0.36374288,  0.32895902,  0.3072981 ],\n",
       "       [ 0.36372867,  0.32896884,  0.30730249],\n",
       "       [ 0.36371955,  0.32898019,  0.30730026],\n",
       "       [ 0.36372085,  0.32897684,  0.30730232],\n",
       "       [ 0.36372284,  0.32897209,  0.30730508],\n",
       "       [ 0.36372323,  0.32897128,  0.30730549],\n",
       "       [ 0.36373949,  0.32896018,  0.30730032],\n",
       "       [ 0.36372275,  0.32897458,  0.30730266],\n",
       "       [ 0.36372074,  0.32897603,  0.30730323],\n",
       "       [ 0.36372148,  0.32897451,  0.30730401],\n",
       "       [ 0.3637311 ,  0.32897206,  0.30729683],\n",
       "       [ 0.36373266,  0.32896965,  0.30729769],\n",
       "       [ 0.36372088,  0.32897582,  0.3073033 ],\n",
       "       [ 0.36372312,  0.3289713 ,  0.30730557],\n",
       "       [ 0.36372496,  0.32897072,  0.30730432],\n",
       "       [ 0.36372257,  0.32897281,  0.30730462],\n",
       "       [ 0.36372596,  0.32897078,  0.30730326],\n",
       "       [ 0.36372099,  0.32897577,  0.30730323],\n",
       "       [ 0.36372254,  0.32897274,  0.30730473],\n",
       "       [ 0.36372093,  0.32897651,  0.30730256],\n",
       "       [ 0.36372283,  0.32897547,  0.3073017 ],\n",
       "       [ 0.36372385,  0.32897333,  0.30730283],\n",
       "       [ 0.36372208,  0.3289734 ,  0.30730452],\n",
       "       [ 0.36372241,  0.32897278,  0.30730481],\n",
       "       [ 0.36372349,  0.3289731 ,  0.30730341],\n",
       "       [ 0.36372096,  0.32897763,  0.30730142],\n",
       "       [ 0.36372204,  0.32897606,  0.3073019 ],\n",
       "       [ 0.36372459,  0.32897696,  0.30729845],\n",
       "       [ 0.36373209,  0.32896696,  0.30730094],\n",
       "       [ 0.36371906,  0.32897976,  0.30730118],\n",
       "       [ 0.36371905,  0.32897992,  0.30730103],\n",
       "       [ 0.36372144,  0.32897478,  0.30730378],\n",
       "       [ 0.36372098,  0.32897556,  0.30730346],\n",
       "       [ 0.36372869,  0.32896832,  0.30730299],\n",
       "       [ 0.36372212,  0.32897494,  0.30730294],\n",
       "       [ 0.36372133,  0.32897567,  0.307303  ],\n",
       "       [ 0.36371987,  0.32897839,  0.30730174],\n",
       "       [ 0.36373989,  0.32896265,  0.30729746],\n",
       "       [ 0.36372275,  0.32897188,  0.30730537],\n",
       "       [ 0.36372121,  0.32897649,  0.3073023 ],\n",
       "       [ 0.36372219,  0.32897414,  0.30730366],\n",
       "       [ 0.36372289,  0.32897295,  0.30730415],\n",
       "       [ 0.36372071,  0.32897726,  0.30730204],\n",
       "       [ 0.36372416,  0.32897278,  0.30730305],\n",
       "       [ 0.36373383,  0.32896345,  0.30730271],\n",
       "       [ 0.36372027,  0.32897761,  0.30730212],\n",
       "       [ 0.36371914,  0.32898001,  0.30730086],\n",
       "       [ 0.36372696,  0.32897083,  0.30730221],\n",
       "       [ 0.36372233,  0.32897361,  0.30730406],\n",
       "       [ 0.36373329,  0.32896622,  0.30730049],\n",
       "       [ 0.36372069,  0.32897649,  0.30730282],\n",
       "       [ 0.36372008,  0.32897987,  0.30730004],\n",
       "       [ 0.36373377,  0.3289716 ,  0.30729463],\n",
       "       [ 0.36372507,  0.32897059,  0.30730434],\n",
       "       [ 0.36372342,  0.32897051,  0.30730607],\n",
       "       [ 0.36372887,  0.32897355,  0.30729758],\n",
       "       [ 0.36372059,  0.32897713,  0.30730228],\n",
       "       [ 0.36372814,  0.32897008,  0.30730179],\n",
       "       [ 0.36373779,  0.32896225,  0.30729996],\n",
       "       [ 0.36372187,  0.32897527,  0.30730286],\n",
       "       [ 0.3637186 ,  0.32898061,  0.30730079],\n",
       "       [ 0.36373054,  0.32896705,  0.30730241],\n",
       "       [ 0.36372039,  0.32897685,  0.30730276],\n",
       "       [ 0.36372376,  0.32897086,  0.30730538],\n",
       "       [ 0.3637267 ,  0.32897793,  0.30729537],\n",
       "       [ 0.36372296,  0.32897187,  0.30730517],\n",
       "       [ 0.36372368,  0.32897427,  0.30730205],\n",
       "       [ 0.36372853,  0.32896986,  0.30730161],\n",
       "       [ 0.36371873,  0.32898236,  0.30729892],\n",
       "       [ 0.36372831,  0.32897253,  0.30729916],\n",
       "       [ 0.36372157,  0.32897432,  0.30730412],\n",
       "       [ 0.36372489,  0.32897335,  0.30730176],\n",
       "       [ 0.36372176,  0.32897394,  0.30730429],\n",
       "       [ 0.36372093,  0.32897645,  0.30730262],\n",
       "       [ 0.36372571,  0.32897265,  0.30730165],\n",
       "       [ 0.36372319,  0.32897144,  0.30730537],\n",
       "       [ 0.3637276 ,  0.32897366,  0.30729874],\n",
       "       [ 0.36372236,  0.32897283,  0.30730481],\n",
       "       [ 0.36372475,  0.32897101,  0.30730423],\n",
       "       [ 0.36372417,  0.32897107,  0.30730476],\n",
       "       [ 0.36372223,  0.32897442,  0.30730335],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746],\n",
       "       [ 0.3637246 ,  0.32896794,  0.30730746]])"
      ]
     },
     "execution_count": 768,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    feed_forward(X, Y, no_neurons_layer)    \n",
    "    backpropagation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
